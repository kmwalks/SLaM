#Install and Load Libraries 

install.packages("car")
install.packages("sm")
install.packages("ggplot2") # for general plots
install.packages("MASS") # for polr package
install.packages("generalhoslem") # for testing model fit (lipsitz test and two others)
install.packages("qwraps2") # for summary_table use
install.packages("quantreg") # testing quantile plots (geom_quantile) and quantile regressions
install.packages ("olsrr") #for testing normality
install.packages("ggpubr") #for density plots
install.packages("dplyr")
install.packages("tidyr")
install.packages("lme4")

library(ggplot2) 
library(MASS) 
library(generalhoslem) 
library(qwraps2) 
library(quantreg) 
library(olsrr)
library(car)
library(sm)
library(ggpubr)
library(dplyr)
library(tidyr)
library(lme4)

#Import Data 
      setwd("~/Desktop")   
      Map <- read.csv("Mapping_Coding_KW_200414.csv", na.strings = "N/A")
      View(Map) #224 kids  

## BASIC DEMOGRAPHICS ## 
      Map_Demo <- subset(Map, Map$Age_Rounded>=4.5 & Map$Age_Rounded<=10 | is.na(Map$Age_Rounded))
      View(Map_Demo) # 220 kids total ages 4.5-9 (including kids we never tested)

#FOR DEMOGRAPHICS OF ALL POSSIBLE KIDS INCLUDING FOR MAPPING 
      Total_Inc_Study <- subset(Map_Demo, Map_Demo$Including.in.Study == "Yes" & Map_Demo$Coded. == "Yes" & Map_Demo$Mapping_Include. == "Yes")
      View(Total_Inc_Study)
      # 200 total kids included in study  
      
      ## Included OVERALL: Numbers of Kids Per Group ##
      EE_Inc <- subset(Total_Inc_Study, Total_Inc_Study$Group_4cat == "English Early")
      View(EE_Inc) #48
      EL_Inc <- subset(Total_Inc_Study, Total_Inc_Study$Group_4cat == "English Later")
      View(EL_Inc) #47
      AE_Inc <- subset(Total_Inc_Study, Total_Inc_Study$Group_4cat == "ASL Early")
      View(AE_Inc) #50
      AL_Inc <- subset(Total_Inc_Study, Total_Inc_Study$Group_4cat == "ASL Later")
      View(AL_Inc) #55
      
      Total_Not_Inc_Study <- subset(Map_Demo, Map_Demo$Mapping_Include. == "No")
      nrow(Total_Not_Inc_Study)
      View(Total_Not_Inc_Study)
      # 20 total kids not included in mapping analyses (if not included in study overall, not including in mapping)
                     
      Total_NotTested_NA <- subset(Map_Demo,  Map_Demo$Tested == "No" | Map_Demo$Tested == "Not Yet" | is.na(Map_Demo$Including.in.Study))
      View(Total_NotTested_NA)
      # 15 Not Tested 
      
      NotTested_Not_Inc <- subset(Total_Not_Inc_Study, Total_Not_Inc_Study$Tested == "No")
      nrow(NotTested_Not_Inc)
      #15 remaining kids listed as not included were not tested
      
      Tested_Not_Inc <- subset(Total_Not_Inc_Study, Total_Not_Inc_Study$Tested == "Yes")
      View(Tested_Not_Inc)
      # 5 kid out of 15 not included in study were tested
      #3 had additional disabilities/suspected disabilities, 1 refused to participate, 1 due to technical difficulties (not filmed entirely)

      ## Tested But Not Included: How many from each group? ## 
            EE_Not_Inc <- subset(Tested_Not_Inc, Tested_Not_Inc$Group_4cat == "English Early")
            nrow(EE_Not_Inc) # 0 of 5 kids tested but not included are EE
            EL_Not_Inc <- subset(Tested_Not_Inc, Tested_Not_Inc$Group_4cat == "English Later")
            nrow(EL_Not_Inc) # 0 of 5 kids tested but not included are EL
            AE_Not_Inc <- subset(Tested_Not_Inc, Tested_Not_Inc$Group_4cat == "ASL Early")
            nrow(AE_Not_Inc)# 3 of 5 kids tested but not included are AE
            AL_Not_Inc <- subset(Tested_Not_Inc, Tested_Not_Inc$Group_4cat == "ASL Later")
            nrow(AL_Not_Inc) # 2 of 5 kids tested but not included are AL
   

## Creating a Table/"Matrix": Included vs Not (& not tested) by Group ##

      rnames <- c("Including in Study","Not Including in Study")
      cnames <- c("English Early","English Later","ASL Early","ASL Later")
      Table_IncStudy <- matrix(c(nrow(EE_Inc),nrow(EL_Inc),nrow(AE_Inc),nrow(AL_Inc),nrow(EE_Not_Inc),nrow(EL_Not_Inc),nrow(AE_Not_Inc),nrow(AL_Not_Inc)), nrow=2, ncol=4, byrow=TRUE, dimnames=list(rnames,cnames))
      Table_IncStudy

## Chi-Square on Table/"Matrix": Does the status of including in study depend on which group kids were in?, aka Early ASL more likely to not be included? ##
chisq.test(Table_IncStudy)
# Test statistic is X-squared = 4.9604, df = 3, p-value = 0.1747, do not reject null, conclude that status of inclusion in study is not dependent on group 4 category

# Find observations with missing Age and SES values
      sum(is.na(Total_Inc_Study$Age_Rounded) | Total_Inc_Study$Age_Rounded=="N/A")
      #Not missing any age values

      sum(is.na(Total_Inc_Study$SES) | Total_Inc_Study$SES=="N/A")
      #Missing 9 SES values
      
      Incorrect_SES <- subset(Total_Inc_Study, Total_Inc_Study$SES < 3) 
      nrow(Incorrect_SES) #NONE ARE INAPPROPRIATE
 
      
# SEX 
  sum(Total_Inc_Study$M.F=='Female') # 103
  sum(Total_Inc_Study$M.F=='Male') # 97
  sum(EE_Inc$M.F=='Female') # 25
  sum(EE_Inc$M.F=='Male') # 23
  sum(EL_Inc$M.F=='Female') # 27
  sum(EL_Inc$M.F=='Male') # 20
  sum(AE_Inc$M.F=='Female') # 28
  sum(AE_Inc$M.F=='Male') # 22
  sum(AL_Inc$M.F=='Female') # 23
  sum(AL_Inc$M.F=='Male') #32

# AGE DESCRIPTIVE STATISTICS
min(EE_Inc$Age_Rounded, na.rm=TRUE) # 4.8
min(EL_Inc$Age_Rounded, na.rm=TRUE) #4.5
min(AE_Inc$Age_Rounded, na.rm=TRUE) # 5.2
min(AL_Inc$Age_Rounded, na.rm=TRUE) # 5.1
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, min) #Creates a table in one 

max(EE_Inc$Age_Rounded, na.rm=TRUE) # 9.7
max(EL_Inc$Age_Rounded, na.rm=TRUE) # 9.8
max(AE_Inc$Age_Rounded, na.rm=TRUE) # 9.9
max(AL_Inc$Age_Rounded, na.rm=TRUE) # 9.8
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, max) 

median(EE_Inc$Age_Rounded, na.rm=TRUE) # 6.7
median(EL_Inc$Age_Rounded, na.rm=TRUE) # 6.2
median(AE_Inc$Age_Rounded, na.rm=TRUE) # 7.3
median(AL_Inc$Age_Rounded, na.rm=TRUE) # 7.4
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, median)

mean(EE_Inc$Age_Rounded, na.rm=TRUE) # 6.87
mean(EL_Inc$Age_Rounded, na.rm=TRUE) # 6.41
mean(AE_Inc$Age_Rounded, na.rm=TRUE) # 7.43
mean(AL_Inc$Age_Rounded, na.rm=TRUE) # 7.47
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, mean)

sd(EE_Inc$Age_Rounded, na.rm=TRUE) # 1.33
sd(EL_Inc$Age_Rounded, na.rm=TRUE) # 1.33
sd(AE_Inc$Age_Rounded, na.rm=TRUE) # 1.51
sd(AL_Inc$Age_Rounded, na.rm=TRUE) # 1.46
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, sd)

range(EE_Inc$Age_Rounded, na.rm=TRUE) # 4.8 9.7
range(EL_Inc$Age_Rounded, na.rm=TRUE) # 4.5 9.8
range(AE_Inc$Age_Rounded, na.rm=TRUE) # 5.2 9.9
range(AL_Inc$Age_Rounded, na.rm=TRUE) # 5.1 9.8
aggregate(Age_Rounded ~ Group_4cat, data=Total_Inc_Study, range)

# SES DESCRIPTIVE STATISTICS
min(EE_Inc$SES, na.rm=TRUE) # 17
min(EL_Inc$SES, na.rm=TRUE) # 3
min(AE_Inc$SES, na.rm=TRUE) # 3
min(AL_Inc$SES, na.rm=TRUE) # 3
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, min)

max(EE_Inc$SES, na.rm=TRUE) # 66
max(EL_Inc$SES, na.rm=TRUE) # 66
max(AE_Inc$SES, na.rm=TRUE) # 62
max(AL_Inc$SES, na.rm=TRUE) # 63.5
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, max)

median(EE_Inc$SES, na.rm=TRUE) # 61
median(EL_Inc$SES, na.rm=TRUE) # 52.75
median(AE_Inc$SES, na.rm=TRUE) # 49.50
median(AL_Inc$SES, na.rm=TRUE) # 47
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, median)

mean(EE_Inc$SES, na.rm=TRUE) # 56.25
mean(EL_Inc$SES, na.rm=TRUE) # 48.25
mean(AE_Inc$SES, na.rm=TRUE) # 43.85
mean(AL_Inc$SES, na.rm=TRUE) # 42.38
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, mean)

sd(EE_Inc$SES, na.rm=TRUE) # 12.32
sd(EL_Inc$SES, na.rm=TRUE) # 13.21
sd(AE_Inc$SES, na.rm=TRUE) # 14.59
sd(AL_Inc$SES, na.rm=TRUE) # 15.97
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, sd)

range(EE_Inc$SES, na.rm=TRUE) # 17 66
range(EL_Inc$SES, na.rm=TRUE) # 3 66
range(AE_Inc$SES, na.rm=TRUE) # 3 62
range(AL_Inc$SES, na.rm=TRUE) # 3 63.5
aggregate(SES ~ Group_4cat, data=Total_Inc_Study, range)

describe(Total_Inc_Study$SES) #overall skewness 
HBE <- subset(Total_Inc_Study, Total_Inc_Study$Group_4cat == "English Early") 
describe(HBE$SES) # skewness of -2.01 (SE =1.8)

#Age of Language Exposure
aggregate(Age.of.Exposure..mo..Language ~ Group_4cat, data=Total_Inc_Study, min)
aggregate(Age.of.Exposure..mo..Language ~ Group_4cat, data=Total_Inc_Study, max)
aggregate(Age.of.Exposure..mo..Language ~ Group_4cat, data=Total_Inc_Study, median)
aggregate(Age.of.Exposure..mo..Language ~ Group_4cat, data=Total_Inc_Study, mean)
aggregate(Age.of.Exposure..mo..Language ~ Group_4cat, data=Total_Inc_Study, sd)

#Race
sum(is.na(Total_Inc_Study$Race)) #16 N/As
nrow(filter(Total_Inc_Study, Race == "White")) #1401
nrow(filter(Total_Inc_Study, Race == "Black or African American")) #11
nrow(filter(Total_Inc_Study, Race == "American Indian or Alaska Native")) #3
nrow(filter(Total_Inc_Study, Race == "Asian")) #10
nrow(filter(Total_Inc_Study, Race == "Native Hawaiian or Other Pacific Islander")) #0
nrow(filter(Total_Inc_Study, Race == "Unsure")) #1
nrow(filter(Total_Inc_Study, Race == "Mixed")) #6
nrow(filter(Total_Inc_Study, Race == "Other")) #12
nrow(filter(Total_Inc_Study, Race == "")) #0

#Ethnicity
nrow(filter(Total_Inc_Study, Ethnicity == "Not Hispanic or Latino")) #118
nrow(filter(Total_Inc_Study, Ethnicity == "Hispanic or Latino")) #35
nrow(filter(Total_Inc_Study, Ethnicity == "")) #0
nrow(filter(Total_Inc_Study, Ethnicity == "Unsure")) #6
sum(is.na(Total_Inc_Study$Ethnicity)) #41

#CREATING TIMING/MODALITY DATA FRAMES
#TIMING GROUPS
Early <- subset(Total_Inc_Study, Total_Inc_Study$Timing == "Early")
Later <- subset(Total_Inc_Study, Total_Inc_Study$Timing == "Later")
#MODALITY GROUPS
ASL <- subset(Total_Inc_Study, Total_Inc_Study$Modality == "ASL")
English <- subset(Total_Inc_Study, Total_Inc_Study$Modality == "English")

#VARIABLE SHORTCUTS#  
Age <- Total_Inc_Study$Age_Rounded
SES <- Total_Inc_Study$SES

## TIMING VIOLIN PLOTS ## 
      TimeGrp <- Total_Inc_Study$Timing
      # TIMING: AGE 
      ggplot(Total_Inc_Study, aes(x=TimeGrp, y=Age)) + geom_violin() + labs(x="Language Timing", y="Age(years)") 
      #VISUALLY VERY SIMILAR DISTRIBUTIONS
      # TIMING: SES
      ggplot(Total_Inc_Study, aes(x=TimeGrp, y=SES)) + geom_violin() + labs(x="Language Timing", y="SES") 
      #VISUALLY VERY SIMILAR DISTRIBUTIONS

## MODALITY VIOLIN PLOTS ##
      ModalityGrp <- Total_Inc_Study$Modality
      # MODALITY: AGE
      ggplot(Total_Inc_Study, aes(x=ModalityGrp, y=Age)) + geom_violin() + labs(x="Language Modality", y="Age(years)")
      #ASL HAS MORE EVEN AGE DISTRIBUTION, ENGLISH HAS < OLDER KIDS
      # MODALITY: SES
      ggplot(Total_Inc_Study, aes(x=ModalityGrp, y=SES)) + geom_violin() + labs(x="Language Modality", y="SES")
      #ENGLISH MORE "TOP HEAVY" FOR SES 
 
## GROUP VIOLIN PLOTS ## 
      Groups <- Total_Inc_Study$Group_4cat
      map.n <- function(x){return(c(y = median(x)*1.05, label = length(x))) }
      mean.n <- function(x){return(c(y= median(x)*0.97, label = round(mean(x), 2)))}   
     #4 GROUPS:AGE
      ggplot(Total_Inc_Study, aes(x=Groups, y=Age, fill = Groups)) + geom_violin() + geom_boxplot(width = 0.2) + labs (x= "Experimental Groups", y = "Age (years)") + stat_summary(fun.data = mean.n, geom = "text", fun.y = median) + stat_summary(fun.data = mean.n, geom= "text", fun.y=mean, color= "gainsboro") + theme(legend.position="none") 
      #Very similar across groups!

      #4 GROUPS:SES
    ggplot(Total_Inc_Study, aes(x=Groups, y=SES, fill = Groups)) + geom_violin() + geom_boxplot(width = 0.2) + labs (x= "Experimental Groups", y = "SES") + stat_summary(fun.data = mean.n, geom = "text", fun.y = median) + stat_summary(fun.data = mean.n, geom= "text", fun.y=mean, color= "gainsboro") + theme(legend.position="none")
    #ENGLISH GROUPS HIGHER SES/"TOP HEAVY"
    
    #4 GROUPS: AGE OF LANGUAGE EXPOSURE
      AgeLang <- Total_Inc_Study$Age.of.Exposure..mo..Language
      ggplot(Total_Inc_Study, aes(x=Groups, y=AgeLang, fill = Groups)) + geom_violin() + geom_boxplot(width = 0.2) + labs (x= "Experimental Groups", y = "Age of Language Exposure (months)") + scale_fill_grey(start = 0.5, end = 0.9) + stat_summary(fun.data = mean.n, geom = "text", fun = median, vjust=-2) + stat_summary(fun.data = mean.n, geom= "text", fun=mean, color= "black", vjust=-2) + theme(legend.position = "none", axis.title.x = element_text(color="black", size=14, family = "Calibri"), axis.text.x = element_text(color="black", size=12, family = "Calibri"), axis.title.y = element_text(color="black", size=14, family = "Calibri"), axis.text.y = element_text(color="black", size=12, family = "Calibri"))
 
#Timing of language exposure between two later groups, significantly different?
      #Unpaired two-sample t-test resource: http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r
      t.test(EL$Age.of.Exposure..mo..Language,AL$Age.of.Exposure..mo..Language, paired = FALSE, alternative = "two.sided")
      #Welch Two Sample t-test, t = -7.7154, df = 82.959, p-value = 2.398e-11 (significantly different!)

# ANOVA for mean Age and mean SES between groups - is the avergae age or SES sig different between groups?
#AGE#
      # mean Age
      # Ho : mu1=...=mu4
      # Ha : mu2=...≠...mu4
      # alpha level: 0.05     
# RUN THAT ANOVA 
Age_ANOVA <- aov(Total_Inc_Study$Age_Rounded ~ Total_Inc_Study$Group_4cat)
summary(Age_ANOVA) #p= 0.000379 #NOT EQUAL BETWEEN GROUPS!
      # If > 0.5, conclude that population means for Age are equal between groups
      # If <0.05, reject null, conclude that population means for Age are not equal between groups, which ones differ?
            #Post-Hoc
                  par(ask=TRUE)
                  opar <- par(no.readonly=TRUE) 
                  TukeyHSD(Age_ANOVA)
                  par(las=1) 
                  par(mar=c(5,8,4,2)) 
                  plot(TukeyHSD(Age_ANOVA))
                  par(opar)

# Assumptions to be able to USE ANOVA  
      #1. Normality   - NOT PASS
      qqPlot(Age_ANOVA) #qq plot: scatterplot of observed vs expected normality and want linear line 
      Age_residuals <- residuals(Age_ANOVA) #residual: all data's mean then overall mean subtracted from each data point (indiv-datasetmean)
      shapiro.test(Age_residuals) #do not reject null, assume normality aka bell curve? WANT THAT HIGH P = normally distributed
      #W = 0.95929, p-value = 1.66e-05, NOT NORMALLY DISTRIBUTED!
      ggdensity(Total_Inc_Study$Age_Rounded, main = "Density plot of AGE", xlab = "Age (years)") #NOT A BELL CURVE!
      #BY GROUPS:
      shapiro.test(EE_Inc$Age_Rounded) #p-value = 0.02365<0.05, reject null, can't assume normality
      shapiro.test(AE_Inc$Age_Rounded)#p-value = 0.005251<0.05, reject null, can't assume normality
      shapiro.test(EL_Inc$Age_Rounded)#p-value = 0.002507<0.05, reject null, can't assume normality
      shapiro.test(AL_Inc$Age_Rounded)#p-value = 0.009083<0.05, reject null, can't assume normality

      #2. Homeogeneity of Variances Between Groups    -PASS
      bartlett.test(Total_Inc_Study$Age_Rounded, Total_Inc_Study$Group_4cat) 
      #Bartlett's K-squared = 1.2209, df = 3, p-value = 0.748, HAVE HOMOGENEITY
            # IF p > 0.5 WANT THAT HIGH P! = HOMOGENEITY ACHIEVED
            # IF p<0.05, reject the null hypothesis, do not assume variances are equal, is the variance for each of the groups similar? 
      plot(Age_ANOVA,1) #x-axis all indiv group means and for each group mean has observations and how spread from group mean, want bands to have same spread aka start and end points 
      #PLOT LOOKS GOOD, SIMILAR SPREAD
     
      #BY GROUPS:
      bartlett.test(Age_Rounded ~ Group_4cat, data=Total_Inc_Study) #p-value = 0.748
      leveneTest(Age_Rounded ~ Group_4cat, data=Total_Inc_Study) #p-value = 0.4146

# IF SHAPIRO AND BARLETT ARE >.O5 YAH GOOD, IF NOT: LEVENE AS BACKUP FOR BARLETT, IF STILL NAH, NON-PARAMETRIC METHOD NEEDS TO BE USED / NOT ANOVA 
      # Violated variances assumption, re-run using kruskal-wallis test (non-parametric)
      # IF ASSUMPTIONS DO NOT HOLD, USE KRUSKAL NON-PARAMATRIC TEST/PAIRWISE.WILCOX.TEST FOR POST-HOC)
      kruskal.test(Total_Inc_Study$Age_Rounded ~ Group_4cat, data = Total_Inc_Study)
      #Kruskal-Wallis chi-squared = 17.831, df = 3, p-value = 0.0004767
      pairwise.wilcox.test(Total_Inc_Study$Age_Rounded, Total_Inc_Study$Group_4cat, p.adjust.method="none", paired = FALSE)

*********************************
#SES#
            # mean SES
            # Ho : mu1=...=mu4
            # Ha : mu2=...≠...mu4
            # alpha level: 0.05
            # Assumptions

# All samples drawn independently
SES_ANOVA <- aov(Total_Inc_Study$SES ~ Total_Inc_Study$Group_4cat)
summary(SES_ANOVA) # p = 6.02e-06 #SES MEANS ARE NOT EQUAL BETWEEN GROUPS
      # p=IF>0.05, do not reject null, conclude that population means for SES are equal between groups

      #If different, perform Post-Hoc: 
      par(ask=TRUE)
      opar <- par(no.readonly=TRUE) 
      TukeyHSD(SES_ANOVA)
      par(las=1) 
      par(mar=c(5,8,4,2)) 
      plot(TukeyHSD(SES_ANOVA))
      par(opar)

      #1. Normality  - FAIL
      qqPlot(SES_ANOVA)
      SES_residuals <- residuals(SES_ANOVA) 
      shapiro.test(SES_residuals) #p-value = 1.021e-11 #NOT NORMALLY DISTRIBUTED
      ggdensity(Total_Inc_Study$SES, main = "Density plot of SES", xlab = "SES") #RIGHT SKEWED
      #BY GROUPS: #ALSO ALL < 0.05
            shapiro.test(EE_Inc$SES)
            shapiro.test(EL_Inc$SES) 
            shapiro.test(AE_Inc$SES) 
            shapiro.test(AL_Inc$SES) 
  
      #2. Homeogeneity of Variances Between Groups - PASS
      bartlett.test(Total_Inc_Study$SES, Total_Inc_Study$Group_4cat) # p >0.05, do not reject the null hypothesis, assume variances are equal
      plot(SES_ANOVA,1) #x-axis all indiv group means and for each group mean has observations and how spread from group mean, want bands to have same spread aka start and end points 
      
      bartlett.test(SES ~ Group_4cat, data=Total_Inc_Study) # p-value = 0.2997
      
      leveneTest(SES ~ Group_4cat, data=Total_Inc_Study) #p-value = 0.05584

# IF FAIL ABOVE ASSUMPTIONS, SWITCH TO NON-PARAMETRIC TEST # 
kruskal.test(SES ~ Group_4cat, data = Total_Inc_Study)
pairwise.wilcox.test(Total_Inc_Study$SES,Total_Inc_Study$Group_4cat,p.adjust.method="none")

*************************************************************************************
#Modality
Age_M_ANOVA <- aov(Total_Inc_Study$Age_Rounded ~ Total_Inc_Study$Modality)
summary(Age_M_ANOVA)  #p=7.01e-05 (not equal!)
Age_M_residuals <- residuals(Age_M_ANOVA)
shapiro.test(Age_M_residuals) #Shapiro-Wilk normality test = W = 0.95778, p-value = 1.147e-05
kruskal.test(Total_Inc_Study$Age_Rounded ~ Total_Inc_Study$Modality, data = Total_Inc_Study) #p-value = 0.0001087

SES_M_ANOVA <- aov(Total_Inc_Study$SES ~ Total_Inc_Study$Modality)
summary(SES_M_ANOVA)  #p=1.24e-05 (not equal)
SES_M_residuals <- residuals(SES_M_ANOVA)
shapiro.test(SES_M_residuals)  #Shapiro-Wilk normality test = W = 0.88351, p-value = 5.155e-11

#Timing
Age_T_ANOVA <- aov(Total_Inc_Study$Age_Rounded ~ Total_Inc_Study$Timing)
summary(Age_T_ANOVA) #p = 0.398, NO DIFFERENCE!
Age_T_residuals <- residuals(Age_T_ANOVA) 
shapiro.test(Age_T_residuals) #Shapiro-Wilk normality test = W = 0.94401, p-value = 5.201e-07
kruskal.test(Total_Inc_Study$Age_Rounded ~ Total_Inc_Study$Timing, data = Total_Inc_Study) 

SES_T_ANOVA <- aov(Total_Inc_Study$SES ~ Total_Inc_Study$Timing)
summary(SES_T_ANOVA)  #p = 0.0213, different
SES_T_residuals <- residuals(SES_T_ANOVA)
shapiro.test(SES_T_residuals) #Shapiro-Wilk normality test = W = 0.86037, p-value = 3.031e-12
kruskal.test(Total_Inc_Study$SES ~ Total_Inc_Study$Timing, data = Total_Inc_Study) #p-value = 0.001756


*************************************************************************************
#Analysis

## MAPPING DATA FRAME ## 
  Map_Inc <- Total_Inc_Study #renaming for ease
  View(Map_Inc) #200 kids

#MIXED EFFECTS LOGISTIC REGRESSION
            Map_long <- pivot_longer(data = Map_Inc, cols=c(ends_with("Answer")), names_to = c("Type"), values_to = "Quantity")
            View(Map_long) 
            Map_long2 <- pivot_longer(data = Map_Inc, cols=c(ends_with("Correct.")), names_to = c("Type2"), values_to = "Correct")
            View(Map_long2)
            Map_longest <- cbind(Map_long, Map_long2)
            View(Map_longest)
            Map_short <- Map_longest[,!grepl("^Item",names(Map_longest))] # delete columns that contain “item” 
            View(Map_short)
            which(colnames(Map_short)=="Child_LanguageTested") #19th column
            which(colnames(Map_short)=="Quantity") #73rd column, WANT TO KEEP THIS COLUMN 
            Map_short <- Map_short[, -c(19:72)] #removes unnecessary columns
            View(Map_short)
            which(colnames(Map_short)=="Comments.1") #20
            which(colnames(Map_short)=="Sum_Numeral.Word_Word.Numeral.1") #90
            Map_short <- Map_short[, -c(20:90)] #removes unnecessary columns
            View(Map_short)

            # From Type column, create a new column (Map_Pair)
            #Create new column for which IF Map_short$Type contains “NW” or “WN”, assign value “Numeral-Word”
            Map_Pair <- c()
            for(i in 1:10200)
            Map_Pair[i] <- ifelse(grepl("NW", Map_short$Type[i]), "Numeral-Word", ifelse(grepl("WN", Map_short$Type[i]), "Numeral-Word", ifelse(grepl("QN", Map_short$Type[i]), "Quantity-Numeral", ifelse(grepl("NQ", Map_short$Type[i]), "Quantity-Numeral", ifelse(grepl("QW", Map_short$Type[i]), "Quantity-Word", ifelse(grepl("WQ", Map_short$Type[i]), "Quantity-Word","bananas"))))))
            Map_short <- cbind(Map_short, Map_Pair)
            View(Map_short)

            #From Quantity column, create Small, Med, Large
            Set_Size <- c()
            Set_Size <- case_when(Map_short$Quantity <= 3 ~ 'Small',
                                  Map_short$Quantity > 3 & Map_short$Quantity<= 5 ~ 'Medium',
                                  Map_short$Quantity > 5 ~ 'Large')
            Map_short <- cbind(Map_short,Set_Size)
            View(Map_short)

            #Running Mixed Effects Logistic Regression
            MixReg <- glmer(Correct ~ Age_Rounded + SES + Set_Size + Map_Pair + Age.of.Exposure..mo..Language + (1 | SUBJECT_ID), data = Map_short, family = binomial, control = glmerControl(optimizer ="bobyqa"), nAGQ = 10)
            summary(MixReg)

            #Obtain Model Descriptives
            se <- sqrt(diag(vcov(MixReg)))
            (tab <- cbind(Est = fixef(MixReg), LL = fixef(MixReg) - 1.96 * se, UL = fixef(MixReg) + 1.96 * se))
            exp(tab) #provides odds ratios

