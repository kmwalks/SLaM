#Mapping Analyses, following Hurst Analyses


##If any of the following not already imported need import.packages("[packagename]")

install.packages("tidyverse") # includes ggplot2??
install.packages("ggplot2") # for general plots
install.packages("beeswarm") # for beeswarm plots
install.packages("colorspace") # for fixing colors in plots
install.packages("stargazer") # for pretty regression output tables
install.packages("MASS") # for polr package
install.packages("generalhoslem") # for testing model fit (lipsitz test and two others)
install.packages("qwraps2") # for summary_table use
install.packages("quantreg") # testing quantile plots (geom_quantile) and quantile regressions
install.packages("sure") # package for calculating residuals for ordinal logistic regression (https://journal.r-project.org/archive/2018/RJ-2018-004/RJ-2018-004.pdf)
install.packages("mediation") # package for testing mediation effects
install.packages("gridExtra")
install.packages(“coin”)
install.packages("dplyr")
install.packages("tidyr")


library(tidyverse) 
library(ggplot2)
library(beeswarm)
library(colorspace) 
library(stargazer)
library(MASS)
library(generalhoslem) 
library(qwraps2) 
library(quantreg) 
library(sure) 
library(mediation) 
library(gridExtra)
library(coin)
library(dplyr)
library(tidyr)

setwd("~/Desktop")
getwd()

Mapping <- read.csv("Mapping_Coding_CH_191119.csv", na.strings = "N/A")
 
typeof(Mapping) # when importing using read.csv, resulting obj type is a list (data frame)
View(Mapping)

####HEARING KIDS ONLY####

Map_HBE <- subset(Mapping, Mapping$Including.in.Study == 'Yes' & Mapping$Coded. == "Yes" & Mapping$Hearing_Cat == 'Hearing')
View(Map_HBE)
str(Map_HBE)

#OVERALL 
#HOW MANY AT OR ABOVE CHANCE…
All <- Map_HBE$SumCorrectTotal_All
Ch <- 12.75 #51/4 (at chance score)
Var_Ch <- length(which(All>= Ch)) #number of observations equal to or above chance
N <- nrow(Map_HBE) #number of total observations
(Var_Ch/N)*100 #100% at or above chance

#HOW MANY AT CEILING? 
All <- Map_HBE$SumCorrectTotal_All
C_All <- 51 #ceiling performance 
Var_All <- length(which(All>= C_All)) #number of observations equal to ceiling 
N <- nrow(Map_HBE) #number of total observations
(Var_All/N)*100 #19% at ceiling… 



#Q1: What does mapping performance look like with this age range (5-9)?
  
  #Proportion Correct for each Mapping Type (all HBE kids)
  nq <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Quantity.Numeral)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Numeral-Quantity") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))
  qn <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Numeral.Quantity)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Quantity-Numeral") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))

  nw <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Word.Numeral)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Numeral-Word") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))
  wn <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Numeral.Word)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Word-Numeral") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))

  wq <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Quantity.Word)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Word-Quantity") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))
  qw <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$AvgCorrect_Word.Quantity)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Quantity-Word") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))
  
  grid.arrange(nq,wq,wn, qn,qw,nw, ncol = 3)
  
  #performance not normally distributed, why using wilcoxon tests
  shapiro.test(Map_HBE2$AvgCorrect_Total)

# COMPARE each type to chance - Hurst et al used one-sample Wilcoxon Signed Rank tests (use median and assume roughly normal distribution around MEDIAN)
  wilcox.test(Map_HBE$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 3.211e-09, YES
  wilcox.test(Map_HBE$AvgCorrect_Numeral.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 3.813e-09, YES
  wilcox.test(Map_HBE$AvgCorrect_Quantity.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 2.702e-09, YES
  wilcox.test(Map_HBE$AvgCorrect_Word.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 6.062e-09, YES
  wilcox.test(Map_HBE$AvgCorrect_Word.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 2.301e-09, YES
  wilcox.test(Map_HBE$AvgCorrect_Numeral.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #p-value = 4.516e-10, YES
#GOOD. ALL SIG ABOVE CHANCE. 

# get average RAW NUMBER correct for each mapping type across all kids 
MapAvgCorr <- colMeans(subset(Map_HBE, select = c(SumTotal_QuantityNumeral, SumTotal_NumeralQuantity, SumTotal_QuantityWord, SumTotal_WordQuantity, SumTotal_NumeralWord, SumTotal_WordNumeral)), na.rm = TRUE)
MapAvgCorr

# Mapping types are out of different numbers, so to calculate proportion correct need code below
MapMeans <- c((MapAvgCorr[1]/9), (MapAvgCorr[2]/8), (MapAvgCorr[3]/9), (MapAvgCorr[4]/8), (MapAvgCorr[5]/9), (MapAvgCorr[6]/8))
MapMeans


## 5 and 6 YO vs 7 and UP ##

##AGE GROUPS DECISONS ##
AGEGRP <- Map_HBE$AgeGrp %>% mutate(AGEGRP = case_when(Map_HBE$Age_Rounded >= 5  & Map_HBE$Age_Rounded < 6 ~ '5', Map_HBE$Age_Rounded >= 6  & Map_HBE$Age_Rounded < 7 ~ '6', Map_HBE$Age_Rounded >= 7  & Map_HBE$Age_Rounded < 8 ~ '7', Map_HBE$Age_Rounded >= 8  & Map_HBE$Age_Rounded < 9 ~ '8', Map_HBE$Age_Rounded >= 9  & Map_HBE$Age_Rounded < 10 ~ '9'))
Map_HBE2 <- cbind(Map_HBE, AGEGRP)
View(Map_HBE2)
YO5 <- subset(Map_HBE2, Map_HBE2$AGEGRP =="5")
YO6 <- subset(Map_HBE2, Map_HBE2$AGEGRP =="6")
YO7 <- subset(Map_HBE2, Map_HBE2$AGEGRP =="7")
YO8 <- subset(Map_HBE2, Map_HBE2$AGEGRP =="8")
YO9 <- subset(Map_HBE2, Map_HBE2$AGEGRP =="9")
t.test(YO5$AvgCorrect_Total, YO6$AvgCorrect_Total, paired = FALSE, alternative = "two.sided")  #t = -1.3824, df = 18.414, p-value = 0.1834, 5 vs 6 NOT
t.test(YO5$AvgCorrect_Total, YO7$AvgCorrect_Total, paired = FALSE, alternative = "two.sided")  # 6 vs 7 SIG DIFFERENT
t.test(YO7$AvgCorrect_Total, YO8$AvgCorrect_Total, paired = FALSE, alternative = "two.sided") #7 vs 8 NOT 
t.test(YO7$AvgCorrect_Total, YO9$AvgCorrect_Total, paired = FALSE, alternative = "two.sided") #7 vs 9 NOT
t.test(YO8$AvgCorrect_Total, YO9$AvgCorrect_Total, paired = FALSE, alternative = "two.sided") #8 vs 9 NOT
t.test(YO6$AvgCorrect_Total, YO8$AvgCorrect_Total, paired = FALSE, alternative = "two.sided") # t = -2.2195, df = 8.8544, p-value = 0.05408, BARELY NOT SIG
t.test(YO6$AvgCorrect_Total, YO9$AvgCorrect_Total, paired = FALSE, alternative = "two.sided") #SIG DIFF
# SIG FINDINGS: 5 vs 6 NO, 6 vs 7 YES, 7 vs 8 NO, 7 vs 9 NO, 8 vs 9 NO, 6 vs 8 NO (but barely p = 0.054), 6 vs 9 YES… so good support for 5 and 6 year olds vs 7 year olds and up

#SETTING UP
#create a column for age group
Map_HBE$AgeGrp <- ifelse(Map_HBE$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")

Map56 <- subset(Map_HBE, Map_HBE$AgeGrp =="5- and 6-year-olds")
Map7up <- subset(Map_HBE, Map_HBE$AgeGrp =="7 and up")

#RAW NUMBER correct & PROPORTION correct, THESE ARE FOR TABLES
Map56AvgCorr <- colMeans(subset(Map56, select = c(SumTotal_QuantityNumeral, SumTotal_NumeralQuantity, SumTotal_QuantityWord, SumTotal_WordQuantity, SumTotal_NumeralWord, SumTotal_WordNumeral)), na.rm = TRUE) 
Map56AvgCorr
Map56Means <- c((Map56AvgCorr[1]/9), (Map56AvgCorr[2]/8), (Map56AvgCorr[3]/9), (Map56AvgCorr[4]/8), (Map56AvgCorr[5]/9), (Map56AvgCorr[6]/8))
Map56Means

Map7upAvgCorr <- colMeans(subset(Map7up, select = c(SumTotal_QuantityNumeral, SumTotal_NumeralQuantity, SumTotal_QuantityWord, SumTotal_WordQuantity, SumTotal_NumeralWord, SumTotal_WordNumeral)), na.rm = TRUE) 
Map7upAvgCorr
Map7Means <- c((Map7upAvgCorr[1]/9), (Map7upAvgCorr[2]/8), (Map7upAvgCorr[3]/9), (Map7upAvgCorr[4]/8), (Map7upAvgCorr[5]/9), (Map7upAvgCorr[6]/8))
Map7Means

#ALL SIGNIFICANTLY DIFFERENT FROM CHANCE?, THESE ARE FOR SIGNIFICANCE ON TABLES
# 5 and 6 YO 
wilcox.test(Map56$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.54e-05
wilcox.test(Map56$AvgCorrect_Numeral.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.784e-05
wilcox.test(Map56$SumTotal_QuantityNumeral, Map56$SumTotal_NumeralQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity numeral better than numeral quantity? 
#YES, p-value = p-value = 0.001521

wilcox.test(Map56$AvgCorrect_Quantity.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.612e-05
wilcox.test(Map56$AvgCorrect_Word.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.956e-05
wilcox.test(Map56$SumTotal_QuantityWord, Map56$SumTotal_WordQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity word better than word quantity?
#YES, p-value = 0.0009956

wilcox.test(Map56$AvgCorrect_Word.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 1.905e-05
wilcox.test(Map56$AvgCorrect_Numeral.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 1.396e-05
wilcox.test(Map56$SumTotal_NumeralWord, Map56$SumTotal_WordNumeral, paired=TRUE, exact=FALSE) # ASYMMETRY?  Numeral word better than word numeral?
#YES, p-value = 0.0002018

# 7 YO & >

wilcox.test(Map7up$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 7.534e-06
wilcox.test(Map7up$AvgCorrect_Numeral.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 1.204e-05
wilcox.test(Map7up$SumTotal_QuantityNumeral, Map7up$SumTotal_NumeralQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity numeral better than numeral quantity?
#YES, V = 190, p-value = 6.333e-05

wilcox.test(Map7up$AvgCorrect_Quantity.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 7.573e-06
wilcox.test(Map7up$AvgCorrect_Word.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 1.925e-05
wilcox.test(Map7up$SumTotal_QuantityWord, Map7up$SumTotal_WordQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity word better than word quantity?
#YES, V = 210, p-value = 4.749e-05

wilcox.test(Map7up$AvgCorrect_Word.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 210, p-value = 1.255e-05
wilcox.test(Map7up$AvgCorrect_Numeral.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 2.525e-06
wilcox.test(Map7up$SumTotal_NumeralWord, Map7up$SumTotal_WordNumeral, paired=TRUE, exact=FALSE) # ASYMMETRY?  Numeral word better than word numeral?
#YES, V = 231, p-value = 1.952e-05

##Overall PAIR COMPARISONS:
MapAvgCorr_Pairs <- colMeans(subset(Map_HBE, select = c(Sum_Quantity.Numeral_Numeral.Quantity, Sum_Quantity.Word_Word.Quantity,Sum_Numeral.Word_Word.Numeral)), na.rm = TRUE)
MapProp_Pairs <- c((MapAvgCorr_Pairs[1]/17), (MapAvgCorr_Pairs[2]/17), (MapAvgCorr_Pairs[3]/17))
MapProp_Pairs
##5-6 YO:
Map56_AvgCorr_Pair <- colMeans(subset(Map56, select = c(Sum_Quantity.Numeral_Numeral.Quantity, Sum_Quantity.Word_Word.Quantity,Sum_Numeral.Word_Word.Numeral)), na.rm = TRUE)
Map56_AvgCorr_Pair
Map56_MapProp_Pairs <- c((Map56_AvgCorr_Pair[1]/17), (Map56_AvgCorr_Pair[2]/17), (Map56_AvgCorr_Pair[3]/17))
Map56_MapProp_Pairs
	#COMPARING TO CHANCE
	Map56_QN <- wilcox.test(Map56$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") 
Map56_QW <- wilcox.test(Map56$Sum_Quantity.Word_Word.Quantity, mu = .25, alternative = "greater") 
Map56_NW <- wilcox.test(Map56$Sum_Numeral.Word_Word.Numeral, mu = .25, alternative = "greater") 
Map56_QN
Map56_QW
Map56_NW
##7 + YO:
Map7up_AvgCorr_Pair <- colMeans(subset(Map7up, select = c(Sum_Quantity.Numeral_Numeral.Quantity, Sum_Quantity.Word_Word.Quantity,Sum_Numeral.Word_Word.Numeral)), na.rm = TRUE)
Map7up_MapProp_Pairs <- c((Map7up_AvgCorr_Pair[1]/17), (Map7up_AvgCorr_Pair[2]/17), (Map7up_AvgCorr_Pair[3]/17))
Map7up_MapProp_Pairs
	#COMPARING TO CHANCE
	Map7up_QN <- wilcox.test(Map7up$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") 
Map7up_QW <- wilcox.test(Map7up$Sum_Quantity.Word_Word.Quantity, mu = .25, alternative = "greater") 
Map7up_NW <- wilcox.test(Map7up$Sum_Numeral.Word_Word.Numeral, mu = .25, alternative = "greater") 
Map7up_QN
Map7up_QW
Map7up_NW


## SEPARATE INTO SMALL (1-3), MEDIUM (4-5), LARGE (6-9) QUANTITIES BY AGE, OVERALL NOT SEPARATED BY MAPPINGS ##
     #SMALL:
          #Target 1: Items 1, 10, 18, 35, 44
          #Target 2: Items 2, 19, 27, 36
          #Target 3: Items 6, 15, 26, 29, 41, 47
     #MEDIUM:
          #Target 4: Items 5, 11, 21, 32, 39, 51
          #Target 5: Items 8, 12, 23, 34, 37, 50
     #LARGE:
          #Target 6: Items 3, 16, 24, 31, 43, 49
          #Target 7: Items 9, 14, 22, 30, 38, 46
          #Target 8: Items 7, 17, 25, 28, 40, 48
          #Target 9: Items 4, 13, 20, 33, 42, 45

     # AGES 5 #
     Map56_Sm_AvgCorr <- colMeans(subset(Map56, select = c(Item1_Correct.,Item10_Correct.,Item18_Correct.,Item35_Correct., Item44_Correct.,Item2_Correct.,Item19_Correct.,Item27_Correct.,Item36_Correct.,Item6_Correct.,Item15_Correct.,Item26_Correct.,Item29_Correct.,Item41_Correct.,Item47_Correct.)), na.rm = TRUE) 
     wilcox.test(Map56_Sm_AvgCorr, mu = .25, alternative = "greater") #is mean of small quantities correct for ages 5 and 6 significantly greater than chance (0.25) 
     #YES, V = 120, p-value = 0.0003031
     Map56_Med_AvgCorr <- colMeans(subset(Map56, select = c(Item5_Correct.,Item11_Correct.,Item21_Correct.,Item32_Correct., Item39_Correct.,Item51_Correct.,Item8_Correct.,Item12_Correct.,Item23_Correct.,Item34_Correct.,Item37_Correct.,Item50_Correct.)), na.rm = TRUE) 
     wilcox.test(Map56_Med_AvgCorr, mu = .25, alternative = "greater") #is mean of medium quantities correct for ages 5 and 6 significantly greater than chance (0.25) 
     #YES, V = 78, p-value = 0.001215
     Map56_Lrg_AvgCorr <- colMeans(subset(Map56, select = c(Item3_Correct.,Item16_Correct.,Item24_Correct.,Item31_Correct., Item43_Correct.,Item49_Correct.,Item9_Correct.,Item14_Correct.,Item22_Correct.,Item30_Correct.,Item38_Correct.,Item46_Correct., Item46_Correct., Item7_Correct., Item17_Correct., Item25_Correct.,Item28_Correct., Item40_Correct., Item48_Correct.,Item4_Correct.,Item13_Correct.,Item20_Correct.,Item33_Correct.,Item42_Correct.,Item45_Correct.)), na.rm = TRUE) 
     wilcox.test(Map56_Lrg_AvgCorr, mu = .25, alternative = "greater") #is mean of large quantities correct for ages 5 and 6 significantly greater than chance (0.25) 
     #YES, V = 325, p-value = 6.201e-06

     # AGES 7 & > #
     Map7up_Sm_AvgCorr <- colMeans(subset(Map7up, select = c(Item1_Correct.,Item10_Correct.,Item18_Correct.,Item35_Correct., Item44_Correct.,Item2_Correct.,Item19_Correct.,Item27_Correct.,Item36_Correct.,Item6_Correct.,Item15_Correct.,Item26_Correct.,Item29_Correct.,Item41_Correct.,Item47_Correct.)), na.rm = TRUE) 
     wilcox.test(Map7up_Sm_AvgCorr, mu = .25, alternative = "greater") #is mean of small quantities correct for ages 6 and > significantly greater than chance (0.25) 
     #YES, V = 120, p-value = 0.0001942
     Map7up_Med_AvgCorr <- colMeans(subset(Map7up, select = c(Item5_Correct.,Item11_Correct.,Item21_Correct.,Item32_Correct., Item39_Correct.,Item51_Correct.,Item8_Correct.,Item12_Correct.,Item23_Correct.,Item34_Correct.,Item37_Correct.,Item50_Correct.)), na.rm = TRUE) 
     wilcox.test(Map7up_Med_AvgCorr, mu = .25, alternative = "greater") #is mean of medium quantities correct for ages 6 and > significantly greater than chance (0.25) 
     #YES, V = 78, p-value = 0.001057
     Map7up_Lrg_AvgCorr <- colMeans(subset(Map7up, select = c(Item3_Correct.,Item16_Correct.,Item24_Correct.,Item31_Correct., Item43_Correct.,Item49_Correct.,Item9_Correct.,Item14_Correct.,Item22_Correct.,Item30_Correct.,Item38_Correct.,Item46_Correct., Item46_Correct., Item7_Correct., Item17_Correct., Item25_Correct.,Item28_Correct., Item40_Correct., Item48_Correct.,Item4_Correct.,Item13_Correct.,Item20_Correct.,Item33_Correct.,Item42_Correct.,Item45_Correct.)), na.rm = TRUE) 
     wilcox.test(Map7up_Lrg_AvgCorr, mu = .25, alternative = "greater") #is mean of large quantities correct for ages 5 significantly greater than chance (0.25) 
     #YES, V = 325, p-value = 4.969e-06

## SEPARATE INTO SMALL (1-3), MEDIUM (4-5), LARGE (6-9) QUANTITIES BY AGE, SEPARATED BY MAPPING PAIRS ##
# AGES 5 # 
#SMALL#
wilcox.test(Map56$AvgCorrect_Sm_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for small quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Sm_QW)
#V = 231, p-value = 9.762e-06, mean 0.94
wilcox.test(Map56$AvgCorrect_Sm_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for small quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Sm_QN)
#V = 231, p-value = 7.573e-06, mean 0.97
wilcox.test(Map56$AvgCorrect_Sm_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for small quantities & ages 5 and 6 significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Sm_WN)
#V = 231, p-value = 9.762e-06, mean 0.96

#MEDIUM#
wilcox.test(Map56$AvgCorrect_Med_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for medium quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Med_QW)
#V = 188, p-value = 6.73e-05, mean 0.77
wilcox.test(Map56$AvgCorrect_Med_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for medium quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Med_QN)
#V = 231, p-value = 2.101e-05, mean 0.85
wilcox.test(Map56$AvgCorrect_Med_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for medium quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Med_WN)
#V = 231, p-value = 3.887e-06, mean 0.99

#LARGE#
wilcox.test(Map56$AvgCorrect_Lrg_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for large  quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Lrg_QW)
#V = 231, p-value = 2.918e-05, mean 0.76
wilcox.test(Map56$AvgCorrect_Lrg_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for large quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Lrg_QN)
#V = 231, p-value = 2.899e-05, mean 0.79
wilcox.test(Map56$AvgCorrect_Lrg_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for large quantities & ages 5 and 6  significantly greater than chance (0.25) 
mean(Map56$AvgCorrect_Lrg_WN)
#V = 231, p-value = 2.134e-05, mean 0.93

# AGES 7 and up # 
#SMALL#
wilcox.test(Map7up$AvgCorrect_Sm_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for small quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Sm_QW)
#V = 231, p-value = 5.576e-06, mean 0.98
wilcox.test(Map7up$AvgCorrect_Sm_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for small quantities & ages 7 and up  significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Sm_QN)
#V = 231, p-value = 3.887e-06, mean 0.99
wilcox.test(Map7up$AvgCorrect_Sm_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for small quantities & aages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Sm_WN)
#V = 231, p-value = 3.887e-06, mean 0.99

#MEDIUM#
wilcox.test(Map7up$AvgCorrect_Med_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for medium quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map6up$AvgCorrect_Med_QW)
#V = 231, p-value = 9.762e-06, mean 0.88
wilcox.test(Map7up$AvgCorrect_Med_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for medium quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Med_QN)
#V = 231, p-value = 3.887e-06, mean 0.99
wilcox.test(Map7up$AvgCorrect_Med_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for medium quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Med_WN)
#V = 231, p-value = 5.587e-06, mean 0.96

#LARGE#
wilcox.test(Map7up$AvgCorrect_Lrg_QW, mu = .25, alternative = "greater") # is mean quantity & word correct for large  quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Lrg_QW)
#V = 231, p-value = 1.897e-05, mean 0.94
wilcox.test(Map7up$AvgCorrect_Lrg_QN, mu = .25, alternative = "greater") # is mean quantity & numeral correct for large quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Lrg_QN)
#V = 231, p-value = 1.428e-05, mean 0.96
wilcox.test(Map7up$AvgCorrect_Lrg_WN, mu = .25, alternative = "greater") # is mean word & numeral correct for large quantities & ages 7 and up significantly greater than chance (0.25) 
mean(Map7up$AvgCorrect_Lrg_WN)
#V = 231, p-value = 5.587e-06, mean 0.97

## SCATTER PLOTS FOR MAPPING TYPES BY AGE GROUPS AND SET SIZES ##
#1. NUMERAL TO WORD SCATTER PLOT#
long_setsizes_NW  <- Map_HBE[,-(298)] #getting rid of sum columns to only have avg ones
long_setsizes_NW2  <- pivot_longer(data = long_setsizes_NW, cols=c(ends_with("WordNumeral")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_NW2)
long_setsizes_NW2$AgeGrp <- ifelse(long_setsizes_NW2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_NW2) 
AgeGrps <- long_setsizes_NW2$AgeGrp #creating short variable
long_setsizes_NW2$SetSize <- factor(long_setsizes_NW2$SetSize, levels=c("AvgCorrect_Sm_WordNumeral", "AvgCorrect_Med_WordNumeral", "AvgCorrect_Lrg_WordNumeral")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
NW <- ggplot(long_setsizes_NW2, aes(x=long_setsizes_NW2$SetSize, y=long_setsizes_NW2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Numeral-Word") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_WordNumeral" = "Small", "AvgCorrect_Med_WordNumeral" = "Medium", "AvgCorrect_Lrg_WordNumeral" = "Large"))
NW

#2. WORD TO NUMERAL SCATTER PLOT#
long_setsizes_WN  <- Map_HBE[,-(270)] #getting rid of sum columns to only have avg ones
long_setsizes_WN2  <- pivot_longer(data = long_setsizes_WN, cols=c(ends_with("NumeralWord")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_WN2)
long_setsizes_WN2$AgeGrp <- ifelse(long_setsizes_WN2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_WN2) 
AgeGrps <- long_setsizes_WN2$AgeGrp #creating short variable
long_setsizes_WN2$SetSize <- factor(long_setsizes_WN2$SetSize, levels=c("AvgCorrect_Sm_NumeralWord", "AvgCorrect_Med_NumeralWord", "AvgCorrect_Lrg_NumeralWord")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
WN <- ggplot(long_setsizes_WN2, aes(x=long_setsizes_WN2$SetSize, y=long_setsizes_WN2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Word-Numeral") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_NumeralWord" = "Small", "AvgCorrect_Med_NumeralWord" = "Medium", "AvgCorrect_Lrg_NumeralWord" = "Large"))
WN

#3. WORD TO QUANTITY SCATTER PLOT#
#which(colnames(Map_HBE)=="SumTotal_QuantityWord") #find number of column
long_setsizes_WQ  <- Map_HBE[,-(187)] #getting rid of sum columns to only have avg ones
long_setsizes_WQ2  <- pivot_longer(data = long_setsizes_WQ, cols=c(ends_with("QuantityWord")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_WQ2)
long_setsizes_WQ2$AgeGrp <- ifelse(long_setsizes_WQ2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_WQ2) 
AgeGrps <- long_setsizes_WQ2$AgeGrp #creating short variable
long_setsizes_WQ2$SetSize <- factor(long_setsizes_WQ2$SetSize, levels=c("AvgCorrect_Sm_QuantityWord", "AvgCorrect_Med_QuantityWord", "AvgCorrect_Lrg_QuantityWord")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
WQ <- ggplot(long_setsizes_WQ2, aes(x=long_setsizes_WQ2$SetSize, y=long_setsizes_WQ2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Word-Quantity") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_QuantityWord" = "Small", "AvgCorrect_Med_QuantityWord" = "Medium", "AvgCorrect_Lrg_QuantityWord" = "Large"))
WQ

#4. QUANTITY TO WORD SCATTER PLOT#
which(colnames(Map_HBE)=="SumTotal_WordQuantity")
long_setsizes_QW  <- Map_HBE[,-(239)] #getting rid of sum columns to only have avg ones
long_setsizes_QW2  <- pivot_longer(data = long_setsizes_QW, cols=c(ends_with("WordQuantity")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_QW2)
long_setsizes_QW2$AgeGrp <- ifelse(long_setsizes_QW2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_QW2) 
AgeGrps <- long_setsizes_QW2$AgeGrp #creating short variable
long_setsizes_QW2$SetSize <- factor(long_setsizes_QW2$SetSize, levels=c("AvgCorrect_Sm_WordQuantity", "AvgCorrect_Med_WordQuantity", "AvgCorrect_Lrg_WordQuantity")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
QW <- ggplot(long_setsizes_QW2, aes(x=long_setsizes_QW2$SetSize, y=long_setsizes_QW2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Quantity-Word") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_WordQuantity" = "Small", "AvgCorrect_Med_WordQuantity" = "Medium", "AvgCorrect_Lrg_WordQuantity" = "Large"))
QW

#5. QUANTITY TO NUMERAL SCATTER PLOT#
which(colnames(Map_HBE)=="SumTotal_NumeralQuantity")
long_setsizes_QN  <- Map_HBE[,-(129)] #getting rid of sum columns to only have avg ones
long_setsizes_QN2  <- pivot_longer(data = long_setsizes_QN, cols=c(ends_with("NumeralQuantity")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_QN2)
long_setsizes_QN2$AgeGrp <- ifelse(long_setsizes_QN2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_QN2) 
AgeGrps <- long_setsizes_QN2$AgeGrp #creating short variable
long_setsizes_QN2$SetSize <- factor(long_setsizes_QN2$SetSize, levels=c("AvgCorrect_Sm_NumeralQuantity", "AvgCorrect_Med_NumeralQuantity", "AvgCorrect_Lrg_NumeralQuantity")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
QN <- ggplot(long_setsizes_QN2, aes(x=long_setsizes_QN2$SetSize, y=long_setsizes_QN2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Quantity-Numeral") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_NumeralQuantity" = "Small", "AvgCorrect_Med_NumeralQuantity" = "Medium", "AvgCorrect_Lrg_NumeralQuantity" = "Large"))
QN

#6. NUMERAL TO QUANTITY SCATTER PLOT#
which(colnames(Map_HBE)=="SumTotal_QuantityNumeral")
long_setsizes_NQ  <- Map_HBE[,-(77)] #getting rid of sum columns to only have avg ones
long_setsizes_NQ2  <- pivot_longer(data = long_setsizes_NQ, cols=c(ends_with("QuantityNumeral")), names_to = c("SetSize"), values_to = "Performance") 
View(long_setsizes_NQ2)
long_setsizes_NQ2$AgeGrp <- ifelse(long_setsizes_NQ2$Age_Rounded < 7, "5", "7") #adding age grp column to only have the numbers
View(long_setsizes_NQ2) 
AgeGrps <- long_setsizes_NQ2$AgeGrp #creating short variable
long_setsizes_NQ2$SetSize <- factor(long_setsizes_NQ2$SetSize, levels=c("AvgCorrect_Sm_QuantityNumeral", "AvgCorrect_Med_QuantityNumeral", "AvgCorrect_Lrg_QuantityNumeral")) #SETTING IN ORDER, from small to large
jitter <- position_jitter(width=0.4, height=0.02)
NQ <- ggplot(long_setsizes_NQ2, aes(x=long_setsizes_NQ2$SetSize, y=long_setsizes_NQ2$Performance)) + geom_point(aes(shape=AgeGrps, color=AgeGrps), show.legend = FALSE, position=jitter, size=2)+ stat_smooth(aes(group = AgeGrps, color=AgeGrps), method = "loess", se=FALSE) + labs(x="Set Sizes", y="Numeral-Quantity") + theme(text = element_text(size=20, family="Times New Roman"), legend.position = "none") + coord_cartesian(ylim= c(0.4,1)) + scale_y_continuous(breaks=c(0.5, 0.6, 0.7, 0.8, 0.9, 1))  + scale_color_manual(values=c("black","blue1")) + scale_x_discrete(labels=c("AvgCorrect_Sm_QuantityNumeral" = "Small", "AvgCorrect_Med_QuantityNumeral" = "Medium", "AvgCorrect_Lrg_QuantityNumeral" = "Large"))
NQ

grid.arrange(NQ,WQ,WN,QN,QW,NW, ncol = 3)

#CEILING PERFORMANCE PREVALENCE?#
NW <- Map_HBE$Sum_Numeral.Word_Word.Numeral
C_NW <- 17
Var_NW_2 <- length(which(NW>= C_NW)) #number of observations that are equal to 17 (ceiling)
N <- nrow(Map_HBE)
(Var_NW_2/N)*100 #61.9% AT CEILING
#SCATTERPLOT FOR NUMERAL AND WORD PERFORMANCE#
NW <- ggplot(Map_HBE, aes(x=Map_HBE$Age_Rounded, y=Map_HBE$Sum_Numeral.Word_Word.Numeral)) + geom_point() + geom_smooth(method="loess", se = FALSE) + labs(x="Age (Years)", y="Numeral and Word") + theme(text = element_text(size=11)) + coord_cartesian(xlim = c(5, 10), ylim= c(10,17)) + scale_y_continuous(breaks=c(10, 11, 12, 13, 14, 15,16,17)) + scale_x_continuous(breaks=c(5, 6, 7, 8, 9))
NW
