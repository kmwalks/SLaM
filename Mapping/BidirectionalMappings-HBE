install.packages("tidyverse") # includes ggplot2??
install.packages("ggplot2") # for general plots
install.packages("beeswarm") # for beeswarm plots
install.packages("colorspace") # for fixing colors in plots
install.packages("stargazer") # for pretty regression output tables
install.packages("MASS") # for polr package
install.packages("generalhoslem") # for testing model fit (lipsitz test and two others)
install.packages("qwraps2") # for summary_table use
install.packages("quantreg") # testing quantile plots (geom_quantile) and quantile regressions
install.packages("sure") # package for calculating residuals for ordinal logistic regression (https://journal.r-project.org/archive/2018/RJ-2018-004/RJ-2018-004.pdf)
install.packages("mediation") # package for testing mediation effects
install.packages("gridExtra") # combine graphs into one figure 
install.packages("plyr")
install.packages("reshape2") #restructuring dataframes, ex melting, merging 
install.packages("plotrix")



library(tidyverse) 
library(ggplot2)
library(beeswarm)
library(colorspace) 
library(stargazer)
library(MASS)
library(generalhoslem) 
library(qwraps2) 
library(quantreg) 
library(sure) 
library(mediation) 
library(gridExtra)
library(plyr)
library(reshape2)
library(plotrix)

setwd("~/Desktop")
getwd()

Mapping <- read.csv("Mapping_Coding_CH_191119.csv", na.strings = "N/A")
 
typeof(Mapping) # when importing using read.csv, resulting obj type is a list (data frame)
View(Mapping)

####HEARING KIDS ONLY####

Map_HBE <- subset(Mapping, Mapping$Including.in.Study == 'Yes' & Mapping$Coded. == "Yes" & Mapping$Hearing_Cat == 'Hearing')
View(Map_HBE)
str(Map_HBE)

## IF SEPARATING BY AGE GROUPS ##
Map_HBE$AgeGrp <- ifelse(Map_HBE$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
Map56 <- subset(Map_HBE, Map_HBE$AgeGrp =="5- and 6-year-olds")
Map7up <- subset(Map_HBE, Map_HBE$AgeGrp =="7 and up")


# TESTING AGE GROUP DIFFERENCES #
boxplot(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp,data=Map_HBE, xlab="Age Groups", ylab="Average Performance")
 #TESTING VARIANCES#
 #http://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r
 res.ftest <- var.test(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data = Map_HBE) #NOT SIG… 
 leveneTest(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data=Map_HBE)  #NOT SIG
 fligner.test(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data=Map_HBE) # chi-squared = 3.9292, df = 1, p-value = 0.04745, SIG DIFF
 #Correlations for age and percent correct for each mapping type#
  install.packages("ggpubr")
  library("ggpubr")
  #ALL SIX TYPES
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Word.Quantity, method=c("pearson", "kendall", "spearman")) #t = 2.962, df = 40, p-value = 0.005123
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Word, method=c("pearson", "kendall", "spearman")) #t = 2.6532, df = 40, p-value = 0.01138
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Numeral, method=c("pearson", "kendall", "spearman")) #t = 2.9464, df = 40, p-value = 0.005339
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.7277, df = 40, p-value = 0.000598
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Word, method=c("pearson", "kendall", "spearman")) #t = 3.4694, df = 40, p-value = 0.001264
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Word.Numeral, method=c("pearson", "kendall", "spearman")) #t = 0.801, df = 40, p-value = 0.4279
  #ALL 3 PAIRS
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Numeral_Numeral.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.9084, df = 40, p-value = 0.0003501
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Word_Word.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.4769, df = 40, p-value = 0.001237
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Word_Word.Numeral, method=c("pearson", "kendall", "spearman")) #t = 1.7641, df = 40, p-value = 0.08535


## MEDIAN AND SD FOR EACH MAPPING TYPE #
median(Map_HBE$SumTotal_QuantityWord)
sd(Map_HBE$SumTotal_QuantityWord, na.rm = TRUE)
median(Map_HBE$SumTotal_WordQuantity)
sd(Map_HBE$SumTotal_WordQuantity, na.rm = TRUE)
median(Map_HBE$SumTotal_QuantityNumeral)
sd(Map_HBE$SumTotal_QuantityNumeral, na.rm = TRUE)
median(Map_HBE$SumTotal_NumeralQuantity)
sd(Map_HBE$SumTotal_NumeralQuantity, na.rm = TRUE)
median(Map_HBE$SumTotal_WordNumeral)
sd(Map_HBE$SumTotal_WordNumeral, na.rm = TRUE)
median(Map_HBE$SumTotal_NumeralWord)
sd(Map_HBE$SumTotal_NumeralWord, na.rm = TRUE)

## ASYMMETRIES? ## 
  #QUANTITY-WORD#
      QW<-wilcox.test(Map_HBE$SumTotal_QuantityWord, Map_HBE$SumTotal_WordQuantity, paired = TRUE, exact=FALSE)
      QW
      QW_Zstat<-qnorm(QW$p.value/2)
      QW_Zstat
      abs(QW_Zstat)/sqrt(17) #calculates r value
      
  #QUANTITY-NUMERAL#
      QN<-wilcox.test(Map_HBE$SumTotal_QuantityNumeral, Map_HBE$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE)
      QN
      QN_Zstat<-qnorm(QN$p.value/2)
      QN_Zstat
      abs(QN_Zstat)/sqrt(17)
      
  #WORD-NUMERAL#
      WN<-wilcox.test(Map_HBE$SumTotal_WordNumeral, Map_HBE$SumTotal_NumeralWord, paired = TRUE, exact=FALSE)
      WN
      WN_Zstat<-qnorm(WN$p.value/2)
      WN_Zstat
      abs(WN_Zstat)/sqrt(17)
 
 #Asymmetries x Ages#
QW_56<-wilcox.test(Map56$SumTotal_QuantityWord, Map56$SumTotal_WordQuantity, paired = TRUE, exact=FALSE) #SIG
QW_7up<-wilcox.test(Map7up$SumTotal_QuantityWord, Map7up$SumTotal_WordQuantity, paired = TRUE, exact=FALSE) #SIG
QN_56<-wilcox.test(Map56$SumTotal_QuantityNumeral, Map56$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE) #SIG
QN_7up<-wilcox.test(Map7up$SumTotal_QuantityNumeral, Map7up$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE) #SIG
WN_56<-wilcox.test(Map56$SumTotal_WordNumeral, Map56$SumTotal_NumeralWord, paired = TRUE, exact=FALSE) #SIG
WN_7up<-wilcox.test(Map7up$SumTotal_WordNumeral, Map7up$SumTotal_NumeralWord, paired = TRUE, exact=FALSE) #SIG
 
 
 ## DEVELOMENTAL TRAJECTORY RESULTS FOR FIGURE##
#5 YO, 1-5 QUANTITIES#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
small$AgeGrp <- ifelse(small$Age_Rounded < 6, "5-year-olds", "6 and up")
sm_Map5 <- subset(small, small$AgeGrp =="5-year-olds")
View(sm_Map5)
#MAPPING PAIR COMPARISONS?#
t.test(sm_Map5$smQN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.37 (QW = QN)
t.test(sm_Map5$smQN, sm_Map5$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.024 (NW > QN)
t.test(sm_Map5$smWN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.019 (NW > QW)
mean(sm_Map5$smQN) #0.82
mean(sm_Map5$smQW) #0.77
mean(sm_Map5$smWN) #0.94
#ASYMMETRIES?# 
#DATA FRAME OF SMALL X MEDIUM FOR EACH MAPPING TYPE#
Small_type <- mutate(Map_HBE, smNQ= AvgCorrect_Sm_QuantityNumeral * AvgCorrect_Med_QuantityNumeral, smQN= AvgCorrect_Sm_NumeralQuantity * AvgCorrect_Med_NumeralQuantity, smNW= AvgCorrect_Sm_WordNumeral * AvgCorrect_Med_WordNumeral, sm_WN= AvgCorrect_Sm_NumeralWord * AvgCorrect_Med_NumeralWord, sm_WQ= AvgCorrect_Sm_QuantityWord, AvgCorrect_Med_QuantityWord, sm_QW= AvgCorrect_Sm_WordQuantity, AvgCorrect_Med_WordQuantity)
View(Small_type)
#5 year olds 1-5#
Small_type$AgeGrp <- ifelse(Small_type$Age_Rounded < 6, "5 year-olds", "6 and up")
sm_Map5_type <- subset(Small_type, Small_type$AgeGrp =="5 year-olds") 
View(sm_Map5_type)
sm_QN_5<-wilcox.test(sm_Map5_type$smQN, sm_Map5_type$smNQ, paired = TRUE, exact=FALSE)
sm_QN_5 #p-value = 0.41 #no asymmetry for QN
sm_NW_5<-wilcox.test(sm_Map5_type$sm_WN, sm_Map5_type$smNW, paired = TRUE, exact=FALSE)
sm_NW_5 #p-value = 0.41 #no asymmetry for NW
sm_QW_5<-wilcox.test(sm_Map5_type$sm_QW, sm_Map5_type$sm_WQ, paired = TRUE, exact=FALSE)
sm_QW_5 #p-value = 0.37 #no asymmetry for QW
#7-9 YO, 1-5 QUANTITIES#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
sm_Map7up <- subset(small, small$AgeGrp =="7 and up")
View(sm_Map7up)
#MAPPING PAIR COMPARISONS?#
t.test(sm_Map7up$smQN,sm_Map7up$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.1613 (QW = QN)
t.test(sm_Map7up$smQN, sm_Map7up$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.409 (NW = QN)
t.test(sm_Map7up$smWN, sm_Map7up$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.4187 (NW = QW)
mean(sm_Map7up$smQN) #0.98
mean(sm_Map7up$smQW) #0.9255952
mean(sm_Map7up$smWN) #0.952381
#7-9 YO, 6-9 QUANTITIES#
#CREATE DF#
large <- mutate(Map_HBE, lrgWN= AvgCorrect_Lrg_WN, lrgQN= AvgCorrect_Lrg_QN, lrgQW= AvgCorrect_Lrg_QW)
View(large)
large$AgeGrp <- ifelse(large$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
large_Map7up <- subset(large, large$AgeGrp =="7 and up")
View(large_Map7up)
#MAPPING PAIR COMPARISONS?#
t.test(large_Map7up$lrgQN,large_Map7up$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.3519 (QW = QN)
t.test(large_Map7up$lrgQN, large_Map7up$lrgWN, paired = TRUE, alternative = "two.sided") #p-value = 0.7252 (NW = QN)
t.test(large_Map7up$lrgWN, large_Map7up$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.2424 (NW = QW)
mean(large_Map7up$lrgWN) #0.9704762
mean(large_Map7up$lrgQW) #0.9357143
mean(large_Map7up$lrgQN) #0.9595238
 #5-6 YO, 1-5 QUANTITIES#
#create quantities 1-5 dataframe, adding new columns#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
#5 and  6 year olds with quantities 1-5#
small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
sm_Map56 <- subset(small, small$AgeGrp =="5- and 6-year-olds")
View(sm_Map56)
#MAPPING PAIR COMPARISONS#
t.test(sm_Map56$smQN,sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.217 (QW = QN)
t.test(sm_Map56$smQN, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.007159 (NW > QN)
t.test(sm_Map56$smWN, sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.004537 (NW > QW)
mean(sm_Map56$smQW) #0.7479762
mean(sm_Map56$smWN) #0.9442857
mean(sm_Map56$smQN) #0.8172619
#5-6 YO, 6-9 QUANTITIES#
#CREATE DF#
large <- mutate(Map_HBE, lrgWN= AvgCorrect_Lrg_WN, lrgQN= AvgCorrect_Lrg_QN, lrgQW= AvgCorrect_Lrg_QW)
View(large)
large$AgeGrp <- ifelse(large$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
large_Map56 <- subset(large, large$AgeGrp =="5- and 6-year-olds")
View(large_Map56)
#MAPPING PAIR COMPARISONS#
t.test(large_Map56$lrgQN,large_Map56$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.383 (QW = QN)
t.test(large_Map56$lrgQN,large_Map56$lrgWN, paired = TRUE, alternative = "two.sided")  #p-value = 0.001778 (NW > QN)
t.test(large_Map56$lrgWN,large_Map56$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 8.263e-05 (NW > QW)
mean(large_Map56$lrgWN) #0.9304762
mean(large_Map56$lrgQN) #0.787619
mean(large_Map56$lrgQW) #0.7580952

 
 
# SCATTER PLOTS OF MAPPING PAIRS # 
#SET JITTER#
jitter <- position_jitter(width=0.7, height=0.04) 
QNy <- Map_HBE$SumTotal_QuantityNumeral
QNx <- Map_HBE$SumTotal_NumeralQuantity
QN <- ggplot(Map_HBE, aes(x=QNx, y=QNy)) + geom_point(position=jitter, size=2)+ labs(x="Quantity-to-Numeral", y="Numeral-to-Quantity")  + ggtitle("Quantity-Numeral") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
QN
# to get r-squared https://rcompanion.org/rcompanion/e_01.html #
QNmodel <- lm(QNy~QNx, data=Map_HBE)
summary(QNmodel) 

QWy <- Map_HBE$SumTotal_QuantityWord
QWx <- Map_HBE$SumTotal_WordQuantity
QW <- ggplot(Map_HBE, aes(x=QWx, y=QWy)) + geom_point(position=jitter, size=2)+ labs(x="Quantity-to-Word", y="Word-to-Quantity") + ggtitle("Quantity-Word") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
QW
QWmodel <- lm(QWy~QWx, data=Map_HBE)
summary(QWmodel) 

NWy <- Map_HBE$SumTotal_NumeralWord
NWx <- Map_HBE$SumTotal_WordNumeral
NW <- ggplot(Map_HBE, aes(x=NWx, y=NWy)) + geom_point(position=jitter, size=2)+ labs(x="Numeral-to-Word", y="Word-to-Numeral") + ggtitle("Numeral-Word") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
NW
NWmodel <- lm(NWy~NWx, data=Map_HBE)
summary(NWmodel)
 
 ## HISTOGRAM OF DIFFERENCE SCORES FOR MAPPING PAIRS ##
     #Want columns to start at x axis, but solutions all include changing expand to start at 0. that would remove the blank columns.#
         DH_QW <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Quantity.Word_Word.Quantity)) + geom_bar(aes(y=..count../sum(..count..))) + labs(y="Proportion of Children", x="Quantity-Word > Word-Quantity") + theme(panel.background = element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_QW

         DH_NW <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Numeral.Word_Word.Numeral)) + geom_bar(aes(y=..count../sum(..count..))) + labs(x="Numeral-Word > Word-Numeral") + theme(panel.background = element_blank(), axis.title.y=element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_NW 

         DH_QN <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity)) + geom_bar(aes(y=..count../sum(..count..))) + labs(x="Quantity-Numeral > Numeral-Quantity")+ theme(panel.background = element_blank(), axis.title.y=element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_QN

         grid.arrange(DH_QW, DH_NW, DH_QN, ncol = 3) 
         
     #SKEWNESS, KURTOSIS, MEAN,RANGE,SD,SE#
         install.packages(“psych”)
         library(psych)
         describe(Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity)
         describe(Map_HBE$Difference_Quantity.Word_Word.Quantity)
         describe(Map_HBE$Difference_Numeral.Word_Word.Numeral)
         
         #If skewness is less than -1 or greater than 1, the distribution is highly skewed.
         #If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.
         #If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.
         
     #PERCENT THAT FALLS BETWEEN 1 POINT#
         DiffQW <- Map_HBE$Difference_Quantity.Word_Word.Quantity
         QW_1 <- length(which(DiffQW >= 1 & DiffQW <= 3)) #number of observations that fall 1 point above or below DiffQW median of 2
         N <- nrow(Map_HBE) #gives # of observations
         (QW_1/N)*100

         DiffNW <- Map_HBE$Difference_Numeral.Word_Word.Numeral
         NW_1 <- length(which(DiffNW >= 0 & DiffNW <= 2)) #number of observations that fall 1 point above or below DiffNW median of 1
         N <- nrow(Map_HBE) #gives # of observations
         (NW_1/N)*100

         DiffQN <- Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity
         QN_1 <- length(which(DiffQN >= 0 & DiffQN <= 1)) #number of observations that fall 1 point above or below DiffQN median of 2
         N <- nrow(Map_HBE) #gives # of observations
         (QN_1/N)*100

# TESTING NORMALITY OF DIFFERENCE SCORES # 
shapiro.test(Map_HBE$Difference_Numeral.Word_Word.Numeral)
#ETC FOR TWO OTHER PAIRS

##GROUPED BAR PLOT##
      ##SOURCE: https://onunicornsandgenes.blog/2014/03/19/using-r-barplot-with-ggplot2/##
      ## 1. CREATE DF FOR AGE GRP 5 ##
      AgeGrp <- Map56$AgeGrp
      QW <- mean(Map56$Sum_Quantity.Word_Word.Quantity)
      NW <- mean(Map56$Sum_Numeral.Word_Word.Numeral)
      QN <- mean(Map56$Sum_Quantity.Numeral_Numeral.Quantity)
      age56 <- data.frame(AgeGrp, QW, NW,QN)
      age56.data <- melt(age56, id.vars='AgeGrp')
      View(age56.data)

      ## 2. CREATE DF FOR AGE GRP 6 > ##
      AgeGrp <- Map7up$AgeGrp
      QW <- mean(Map7up$Sum_Quantity.Word_Word.Quantity)
      NW <- mean(Map7up$Sum_Numeral.Word_Word.Numeral)
      QN <- mean(Map7up$Sum_Quantity.Numeral_Numeral.Quantity)
      age7 <- data.frame(AgeGrp, QW,NW,QN)
      age7.data <- melt(age7, id.vars='AgeGrp')
      View(age7.data)

      ## 3. MERGE 2 AGE GROUP DFS ##
      K <- merge(age56.data, age7.data, by=c("AgeGrp","variable","value"), all = T)
      names(K)[names(K) == "variable"] <- "Map"
      View(K)

      ## 4. CREATE NEW DATA FRAME WITH SEMS ##
          #AGE 5 and 6#         
          AgeGrp <- Map56$AgeGrp
          QW_SE <- std.error(Map56$Sum_Quantity.Word_Word.Quantity)
          NW_SE <- std.error(Map56$Sum_Numeral.Word_Word.Numeral)
          QN_SE <- std.error(Map56$Sum_Quantity.Numeral_Numeral.Quantity)
          SE_56 <- data.frame(AgeGrp, QW_SE, NW_SE, QN_SE)
          SE_56 <- melt(SE_56, id.vars='AgeGrp')
          View(SE_56)

         #AGE 7 and up#
         AgeGrp <- Map7up$AgeGrp
         QW_SE <- std.error(Map7up$Sum_Quantity.Word_Word.Quantity)
         NW_SE <- std.error(Map7up$Sum_Numeral.Word_Word.Numeral)
         QN_SE <- std.error(Map7up$Sum_Quantity.Numeral_Numeral.Quantity)
         SE_7 <- data.frame(AgeGrp, QW_SE, NW_SE, QN_SE)
         SE_7 <- melt(SE_7, id.vars='AgeGrp')
         View(SE_7)

      ## 5. MERGE 2 SEM DATA FRAMES & PREP TO MERGE WITH OTHER MERGED DF ##
      K2 <- merge(SE_56, SE_7, by=c("AgeGrp","variable","value"), all = T)
      View(K2)
      K2 <- data.frame(K2$AgeGrp, K2$value, K2$variable, K$Map)
      names(K2)[names(K2) == "K2.value"] <- "value"
      names(K2)[names(K2) == "K2.AgeGrp"] <- "AgeGrp"
      names(K2)[names(K2) == "K.Map"] <- "Map"
      names(K2)[names(K2) == "K2.variable"] <- "variable"
      View(K2)

      ## 6. MERGE PREVIOUSLY COMBINED DATAFRAME WITH SEM DATA FRAME ##
      BOOYAH <- merge(K, K2, by=c("AgeGrp","Map"), all = T)
      names(BOOYAH)[names(BOOYAH) == "value.x"] <- "mean"
      names(BOOYAH)[names(BOOYAH) == "value.y"] <- "sem"
      View(BOOYAH)

      ## 7. ADD COLUMNS FOR ERROR BARS ##
             lower <- BOOYAH$mean - BOOYAH$sem
             upper <- BOOYAH$mean + BOOYAH$sem
             BOOYAH<- cbind(BOOYAH, lower, upper)
             View(BOOYAH)

      ## 8. CREATE BAR PLOT ##
All_Bar <- ggplot(BOOYAH, aes(BOOYAH$Map, BOOYAH$mean)) + geom_bar(aes(fill = BOOYAH$AgeGrp), width = 0.6, position = position_dodge(width=0.7), stat="identity") + scale_fill_manual(values=c("grey87", "grey51"), labels = c("5 and 6-year-olds", "7-year-olds and older")) + labs( y="Accuracy (out of 17)") + scale_y_continuous (breaks=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17), limits = c(0,17), expand = c(0,0)) + scale_x_discrete(limits=c("QN","QW","NW"), expand = c(.2,0),labels = c("Quantity and Numeral", "Quantity and Word", "Numeral and Word")) + theme_bw() + theme(legend.position="top", legend.title = element_blank(), axis.title.x=element_blank(), panel.border = element_blank(),panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.line = element_line(colour = "black"), text = element_text(size=12, family="Times New Roman"))
BARS <- All_Bar + geom_errorbar(aes(ymax=BOOYAH$upper, ymin=BOOYAH$lower, group=AgeGrp), width=.5, position=position_dodge(0.7), data=BOOYAH)
BARS

#TESTING SIGNIFICANCE BETWEEN BARS#
t.test(Map56$Sum_Quantity.Numeral_Numeral.Quantity, Map7up$Sum_Quantity.Numeral_Numeral.Quantity, paired = FALSE, alternative = "two.sided") 
t.test(Map56$Sum_Quantity.Word_Word.Quantity, Map7up$Sum_Quantity.Word_Word.Quantity, paired = FALSE, alternative = "two.sided")
t.test(Map56$Sum_Numeral.Word_Word.Numeral, Map7up$Sum_Numeral.Word_Word.Numeral, paired = FALSE, alternative = "two.sided") 

#BOX PLOT
 AgeGrp <- Map56$AgeGrp
 QW <- Map56$Sum_Quantity.Word_Word.Quantity
 NW <- Map56$Sum_Numeral.Word_Word.Numeral
 QN <- Map56$Sum_Quantity.Numeral_Numeral.Quantity
 age56 <- data.frame(AgeGrp, QW, NW,QN)
 age56.data <- melt(age56, id.vars='AgeGrp')
 View(age56.data)
 AgeGrp <- Map7up$AgeGrp
 QW <- Map7up$Sum_Quantity.Word_Word.Quantity
 NW <- Map7up$Sum_Numeral.Word_Word.Numeral
 QN <- Map7up$Sum_Quantity.Numeral_Numeral.Quantity
 age7 <- data.frame(AgeGrp, QW,NW,QN)
 age7.data <- melt(age7, id.vars='AgeGrp')
 View(age7.data)
 B <- merge(age56.data, age7.data, by=c("AgeGrp","variable","value"), all = T)
 names(B)[names(B) == "variable"] <- "Map"
 View(B)
All_BOX <- ggplot(B, aes(B$Map, B$value, fill=B$AgeGrp)) + geom_boxplot() + scale_fill_manual(values=c("grey87", "grey51"), labels = c("5 and 6-year-olds", "7 to 9-year-olds")) + labs( y="Accuracy (out of 17)") + scale_y_continuous (breaks=c(7,8,9,10,11,12,13,14,15,16,17), limits = c(7,17), expand = c(0,0)) + scale_x_discrete(limits=c("QN","QW","NW"), expand = c(.2,0),labels = c("Quantity-Numeral", "Quantity-Word", "Numeral-Word")) + theme_bw() + theme(legend.position="top", legend.title = element_blank(), axis.title.x=element_blank(), panel.border = element_blank(),panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.line = element_line(colour = "black"), text = element_text(size=16, family="Times New Roman")) 
All_BOX

#BOXPLOT AND VIOLIN#
dodge <- position_dodge(width = 0.5)
All_BV <- ggplot(B, aes(B$Map, B$value, fill=B$AgeGrp)) + geom_violin (position = dodge)+ geom_boxplot(width=0.2, position = dodge) + scale_fill_manual(values=c("grey87", "grey51"), labels = c("5 and 6-year-olds", "7 to 9-year-olds")) + labs( y="Accuracy (out of 17)") + scale_y_continuous (breaks=c(7,8,9,10,11,12,13,14,15,16,17), limits = c(7,17), expand = c(0,0)) + scale_x_discrete(limits=c("QN","QW","NW"), expand = c(.2,0),labels = c("Quantity-Numeral", "Quantity-Word", "Numeral-Word")) + theme_bw() + theme(legend.position="top", legend.title = element_blank(), axis.title.x=element_blank(), panel.border = element_blank(),panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), axis.line = element_line(colour = "black"), text = element_text(size=16, family="Times New Roman")) 
All_BV


## ANOVAS ##
     ## BEFORE RUN, HAVE BOOYAH AS A DF FROM ABOVE CODE ##
     ## REMEMBER TYPE 2 IS FOR BETWEEN SUBJECTS ##
     
     replications(BOOYAH) # CHECK BALANCE OF THE DATA TO ENSURE CAN USE AOV FXN. AOV can be used for balanced & unbalanced, but to a certain extent#

     ## AGE GROUP (2 LEVELS) --> SUM SCORES? One-way ANOVA unbalanced (different sample sizes), between subjects design ##
     aov_age<- aov(BOOYAH$mean ~ BOOYAH$AgeGrp)
     summary(aov_age)
     tuk_age<- TukeyHSD(aov_age)
     tuk_age  #can see WHICH variables are significantly different#

     ## MAP PAIR (3 LEVELS) --> SUM SCORES? One-way ANOVA balanced, within subjects/repeated measures design ##
     aov_map<- aov(BOOYAH$mean ~ BOOYAH$Map)
     summary(aov_map)
     tuk_map<- TukeyHSD(aov_map)
     tuk_map #can see WHICH variables are significantly different#

     ## INTERACTION BETWEEN INDEPENDENT VARIABLES? ##
     ## Source: https://rcompanion.org/handbook/G_09.html ##
     install.packages("car")
     library(car)
     model = lm(BOOYAH$mean ~ BOOYAH$AgeGrp + BOOYAH$Map + BOOYAH$AgeGrp:BOOYAH$Map, data = BOOYAH)
     anova(model)
     
     
     ## ETA-SQUARED, WITH ANOVA=TRUE CAN OBTAIN ANOVA TABLES AS WELL, DOUBLE CHECK ##
     # SOURCE: https://cran.r-project.org/web/packages/lsr/lsr.pdf #
     install.packages("lsr")
     library(lsr)
     
     etaSquared(aov_age,type = 1, anova = TRUE)
     etaSquared(aov_map,type = 1, anova = TRUE)
     etaSquared(model,type = 2, anova = TRUE)
     
     #REGRESSION FOR SES ON MAPPING PERFORMANCE
     SES <- Total_Inc_Study$SES_range_8_to_66 
     REG = lm(Total_Inc_Study$SumCorrectTotal_All ~ SES, data =Total_Inc_Study)
     anova(REG)
     etaSquared(REG,type = 1, anova = TRUE)
   
      
 ## t-tests and Cohen’s d, paired t-test (same sample, different variables) ##
     install.packages("effsize") #compute cohen's d
     library(effsize)
     
     ## MAPPING PAIRS MEANS ##
     mean(Map_HBE$Sum_Quantity.Numeral_Numeral.Quantity)
     mean(Map_HBE$Sum_Numeral.Word_Word.Numeral)
     mean(Map_HBE$Sum_Quantity.Word_Word.Quantity)
     
     ## AGE GROUP MEANS ##
     mean(age7.data$value)
     mean(age56.data$value)
     t.test(Map56$AvgCorrect_Total, Map7up$AvgCorrect_Total, paired = TRUE, alternative = "two.sided") #OLDER AGE GROUP BETTER OVERALL
     cohen.d(Map56$AvgCorrect_Total, Map7up$AvgCorrect_Total,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
     
    ## QN vs QW ##
    t.test(Map_HBE$Sum_Quantity.Numeral_Numeral.Quantity, Map_HBE$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
    cohen.d(Map_HBE$Sum_Quantity.Numeral_Numeral.Quantity,Map_HBE$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)

    ## QN vs NW ##
    t.test(Map_HBE$Sum_Quantity.Numeral_Numeral.Quantity, Map_HBE$Sum_Numeral.Word_Word.Numeral, paired = TRUE, alternative = "two.sided")
    cohen.d(Map_HBE$Sum_Quantity.Numeral_Numeral.Quantity, Map_HBE$Sum_Numeral.Word_Word.Numeral,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
    
    ## NW vs QW ##
    t.test(Map_HBE$Sum_Numeral.Word_Word.Numeral, Map_HBE$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
    cohen.d(Map_HBE$Sum_Numeral.Word_Word.Numeral, Map_HBE$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
    
    #MAPPING PAIRS COMPARING TO HURST, WITH YOUNGER AGES AND QUANITIES 1-5#
    #create quantities 1-5 dataframe, adding new columns#
    small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
    View(small)
    #5 and  6 year olds with quantities 1-5#
    small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
    sm_Map56 <- subset(small, small$AgeGrp =="5- and 6-year-olds")
    View(sm_Map56)
    #5 year olds with quantities 1-5#
    small$AgeGrp <- ifelse(small$Age_Rounded < 6, "5", "6 and up")
    sm_Map5 <- subset(small, small$AgeGrp =="5")
    View(sm_Map5)
    ## QN vs QW ##
    5 and 6 year olds: t.test(sm_Map56$smQN, sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #t = 1.2748, df = 20, p-value = 0.217
    5 year olds: t.test(sm_Map5$smQN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #t = 0.92789, df = 11, p-value = 0.3734
    ## QN vs NW ##
    5 and 6 year olds: t.test(sm_Map56$smQN, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #t = -2.9948, df = 20, p-value = 0.007159, NW HIGHER 
    5 year olds: NW HIGHER 
    ## NW vs QW ##
    5 and 6 year olds: t.test(sm_Map56$smQW, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #t = -3.1961, df = 20, p-value = 0.004537, NW HIGHER
    5 year olds: NW HIGHER
    
    #GENERAL AGE GROUP COMPARISON OF MAPPING PAIRS#
    #QN
    t.test(Map56$Sum_Quantity.Numeral_Numeral.Quantity,Map7up$Sum_Quantity.Numeral_Numeral.Quantity, paired = FALSE, alternative = "two.sided") 
    mean(Map56$Sum_Quantity.Numeral_Numeral.Quantity)
    mean(Map7up$Sum_Quantity.Numeral_Numeral.Quantity) 
    cohen.d(Map56$Sum_Quantity.Numeral_Numeral.Quantity,Map7up$Sum_Quantity.Numeral_Numeral.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
    #QW
    t.test(Map56$Sum_Quantity.Word_Word.Quantity,Map7up$Sum_Quantity.Word_Word.Quantity, paired = FALSE, alternative = "two.sided") 
    mean(Map56$Sum_Quantity.Word_Word.Quantity) 
    mean(Map7up$Sum_Quantity.Word_Word.Quantity) 
    cohen.d(Map56$Sum_Quantity.Word_Word.Quantity,Map7up$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
    #NW
    t.test(Map56$Sum_Numeral.Word_Word.Numeral, Map7up$Sum_Numeral.Word_Word.Numeral, paired = FALSE, alternative = "two.sided") 
    mean(Map56$Sum_Numeral.Word_Word.Numeral) 
    mean(Map7up$Sum_Numeral.Word_Word.Numeral) 
    cohen.d(Map56$Sum_Numeral.Word_Word.Numeral, Map7up$Sum_Numeral.Word_Word.Numeral,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 

    
    
    ## AGE GROUPS: QN vs NW ##
      #5 and 6
      t.test(Map56$Sum_Quantity.Numeral_Numeral.Quantity,Map56$Sum_Numeral.Word_Word.Numeral, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$Sum_Quantity.Numeral_Numeral.Quantity,Map56$Sum_Numeral.Word_Word.Numeral, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #7 & up
      t.test(Map7up$Sum_Quantity.Numeral_Numeral.Quantity,Map7up$Sum_Numeral.Word_Word.Numeral, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$Sum_Quantity.Numeral_Numeral.Quantity,Map7up$Sum_Numeral.Word_Word.Numeral,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
    
    ## AGE GROUPS: QN vs QW ##
       #5 and 6
       t.test(Map56$Sum_Quantity.Numeral_Numeral.Quantity, Map56$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
       cohen.d(Map56$Sum_Quantity.Numeral_Numeral.Quantity, Map56$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       #7 & up
       t.test(Map7up$Sum_Quantity.Numeral_Numeral.Quantity, Map7up$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
       cohen.d(Map7up$Sum_Quantity.Numeral_Numeral.Quantity, Map7up$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       
    ## AGE GROUPS: NW vs QW ##
       #5 and 6
       t.test(Map56$Sum_Numeral.Word_Word.Numeral, Map56$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
       cohen.d(Map56$Sum_Numeral.Word_Word.Numeral, Map56$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       #7 & up
       t.test(Map7up$Sum_Numeral.Word_Word.Numeral, Map7up$Sum_Quantity.Word_Word.Quantity, paired = TRUE, alternative = "two.sided")
       cohen.d(Map7up$Sum_Numeral.Word_Word.Numeral, Map7up$Sum_Quantity.Word_Word.Quantity,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       
mean(Map56$Sum_Numeral.Word_Word.Numeral) 
mean(Map7up$Sum_Numeral.Word_Word.Numeral) 
mean(Map56$Sum_Quantity.Numeral_Numeral.Quantity) 
mean(Map7up$Sum_Quantity.Numeral_Numeral.Quantity) 

mean(Map7up$Sum_Quantity.Numeral_Numeral.Quantity) 
mean(Map56up$Sum_Quantity.Word_Word.Quantity) 

mean(Map56$Sum_Numeral.Word_Word.Numeral)
mean(Map7up$Sum_Numeral.Word_Word.Numeral)
mean(Map56$Sum_Quantity.Word_Word.Quantity) 
mean(Map7up$Sum_Quantity.Word_Word.Quantity)

WHAT IF LOOKED AT NW VS QW:
   t.test(Map_HBE$AvgCorrect_Numeral.Word,Map_HBE$AvgCorrect_Quantity.Word, paired = TRUE, alternative = "two.sided")
   cohen.d(Map_HBE$AvgCorrect_Numeral.Word,Map_HBE$AvgCorrect_Quantity.Word, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
mean(Map_HBE$AvgCorrect_Numeral.Word) #0.98
mean(Map_HBE$AvgCorrect_Quantity.Word) #0.92
AND LOOKED AT WN VS WQ:
   t.test(Map_HBE$AvgCorrect_Word.Numeral,Map_HBE$AvgCorrect_Word.Quantity, paired = TRUE, alternative = "two.sided")
   cohen.d(Map_HBE$AvgCorrect_Word.Numeral,Map_HBE$AvgCorrect_Word.Quantity, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
mean(Map_HBE$AvgCorrect_Word.Quantity) #0.84
mean(Map_HBE$AvgCorrect_Word.Numeral) #0.94

## MAPPING MEANS FOR QUANTITY SIZES ##
      ## QW ##
      mean(Map_HBE$AvgCorrect_Sm_QW)
      mean(Map_HBE$AvgCorrect_Med_QW)
      mean(Map_HBE$AvgCorrect_Lrg_QW)

      ## NW ##
      mean(Map_HBE$AvgCorrect_Sm_WN)
      mean(Map_HBE$AvgCorrect_Med_WN)
      mean(Map_HBE$AvgCorrect_Lrg_WN)

      ## QN ##
      mean(Map_HBE$AvgCorrect_Sm_QN)
      mean(Map_HBE$AvgCorrect_Med_QN)
      mean(Map_HBE$AvgCorrect_Lrg_QN)
       
# SET SIZES T TESTS #
      #GENERAL SMALL VERSUS LARGE DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Lrg_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Lrg_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Lrg_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Lrg_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
      #GENERAL SMALL VERSUS MEDIUM DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Med_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Med_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Med_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Med_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Med_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
       #GENERAL MEDIUM VERSUS LARGE DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Med_QW, Map_HBE$AvgCorrect_Lrg_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Med_QW, Map_HBE$AvgCorrect_Lrg_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Med_QN, Map_HBE$AvgCorrect_Lrg_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Med_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Med_WN, Map_HBE$AvgCorrect_Lrg_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Med_WN, Map_HBE$AvgCorrect_Lrg_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
     
      #SMALL VERSUS LARGE#
       #QW
      t.test(Map56$AvgCorrect_Sm_QW, Map56$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_QW, Map56$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_QW, Map7up$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_QW, Map7up$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
     t.test(Map56$AvgCorrect_Sm_QN, Map56$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
     cohen.d(Map56$AvgCorrect_Sm_QN, Map56$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
     t.test(Map7up$AvgCorrect_Sm_QN, Map7up$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
     cohen.d(Map7up$AvgCorrect_Sm_QN, Map7up$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Sm_WN, Map56$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_WN, Map56$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_WN, Map7up$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_WN, Map7up$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       
      #SMALL VERSUS MEDIUM#
       #QW
       t.test(Map56$AvgCorrect_Sm_QW,Map56$AvgCorrect_Med_QW, paired = TRUE, alternative = "two.sided")
       cohen.d(Map56$AvgCorrect_Sm_QW,Map56$AvgCorrect_Med_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       t.test(Map7up$AvgCorrect_Sm_QW,Map7up$AvgCorrect_Med_QW, paired = TRUE, alternative = "two.sided")
       cohen.d(Map7up$AvgCorrect_Sm_QW,Map7up$AvgCorrect_Med_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
      t.test(Map56$AvgCorrect_Sm_QN,Map56$AvgCorrect_Med_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_QN,Map56$AvgCorrect_Med_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_QN,Map7up$AvgCorrect_Med_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_QN,Map7up$AvgCorrect_Med_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Sm_WN,Map56$AvgCorrect_Med_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_WN,Map56$AvgCorrect_Med_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_WN,Map7up$AvgCorrect_Med_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_WN,Map7up$AvgCorrect_Med_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)

     #MEDIUM VERSUS LARGE#
      #QW
      t.test(Map56$AvgCorrect_Med_QW,Map56$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_QW,Map56$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_QW,Map7up$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_QW,Map7up$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
      t.test(Map56$AvgCorrect_Med_QN,Map56$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_QN,Map56$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_QN,Map7up$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_QN,Map7up$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Med_WN,Map56$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_WN,Map56$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_WN,Map7up$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_WN,Map7up$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
    
       
    
