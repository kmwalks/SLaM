install.packages("tidyverse") # includes ggplot2??
install.packages("ggplot2") # for general plots
install.packages("beeswarm") # for beeswarm plots
install.packages("colorspace") # for fixing colors in plots
install.packages("stargazer") # for pretty regression output tables
install.packages("MASS") # for polr package
install.packages("generalhoslem") # for testing model fit (lipsitz test and two others)
install.packages("qwraps2") # for summary_table use
install.packages("quantreg") # testing quantile plots (geom_quantile) and quantile regressions
install.packages("sure") # package for calculating residuals for ordinal logistic regression (https://journal.r-project.org/archive/2018/RJ-2018-004/RJ-2018-004.pdf)
install.packages("mediation") # package for testing mediation effects
install.packages("gridExtra") # combine graphs into one figure 
install.packages("plyr")
install.packages("reshape2") #restructuring dataframes, ex melting, merging 
install.packages("plotrix")



library(tidyverse) 
library(ggplot2)
library(beeswarm)
library(colorspace) 
library(stargazer)
library(MASS)
library(generalhoslem) 
library(qwraps2) 
library(quantreg) 
library(sure) 
library(mediation) 
library(gridExtra)
library(plyr)
library(reshape2)
library(plotrix)

setwd("~/Desktop")
getwd()

Mapping <- read.csv("Mapping_Coding_CH_191119.csv", na.strings = "N/A")
 
typeof(Mapping) # when importing using read.csv, resulting obj type is a list (data frame)
View(Mapping)

####HEARING KIDS ONLY####

Map_HBE <- subset(Mapping, Mapping$Including.in.Study == 'Yes' & Mapping$Coded. == "Yes" & Mapping$Hearing_Cat == 'Hearing')
View(Map_HBE)
str(Map_HBE)


# TESTING AGE GROUP DIFFERENCES #
boxplot(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp,data=Map_HBE, xlab="Age Groups", ylab="Average Performance")
 #TESTING VARIANCES#
 #http://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r
 res.ftest <- var.test(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data = Map_HBE) #NOT SIG… 
 leveneTest(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data=Map_HBE)  #NOT SIG
 fligner.test(Map_HBE$AvgCorrect_Total~Map_HBE$AgeGrp, data=Map_HBE) # chi-squared = 3.9292, df = 1, p-value = 0.04745, SIG DIFF
 #Correlations for age and percent correct for each mapping type#
  install.packages("ggpubr")
  library("ggpubr")
  #ALL SIX TYPES
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Word.Quantity, method=c("pearson", "kendall", "spearman")) #t = 2.962, df = 40, p-value = 0.005123
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Word, method=c("pearson", "kendall", "spearman")) #t = 2.6532, df = 40, p-value = 0.01138
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Numeral, method=c("pearson", "kendall", "spearman")) #t = 2.9464, df = 40, p-value = 0.005339
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.7277, df = 40, p-value = 0.000598
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Word, method=c("pearson", "kendall", "spearman")) #t = 3.4694, df = 40, p-value = 0.001264
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Word.Numeral, method=c("pearson", "kendall", "spearman")) #t = 0.801, df = 40, p-value = 0.4279
  #ALL 3 PAIRS
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Numeral_Numeral.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.9084, df = 40, p-value = 0.0003501
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Quantity.Word_Word.Quantity, method=c("pearson", "kendall", "spearman")) #t = 3.4769, df = 40, p-value = 0.001237
  cor.test(Map_HBE$Age_Rounded, Map_HBE$AvgCorrect_Numeral.Word_Word.Numeral, method=c("pearson", "kendall", "spearman")) #t = 1.7641, df = 40, p-value = 0.08535


## MEDIAN AND SD FOR EACH MAPPING TYPE #
median(Map_HBE$SumTotal_QuantityWord)
sd(Map_HBE$SumTotal_QuantityWord, na.rm = TRUE)
median(Map_HBE$SumTotal_WordQuantity)
sd(Map_HBE$SumTotal_WordQuantity, na.rm = TRUE)
median(Map_HBE$SumTotal_QuantityNumeral)
sd(Map_HBE$SumTotal_QuantityNumeral, na.rm = TRUE)
median(Map_HBE$SumTotal_NumeralQuantity)
sd(Map_HBE$SumTotal_NumeralQuantity, na.rm = TRUE)
median(Map_HBE$SumTotal_WordNumeral)
sd(Map_HBE$SumTotal_WordNumeral, na.rm = TRUE)
median(Map_HBE$SumTotal_NumeralWord)
sd(Map_HBE$SumTotal_NumeralWord, na.rm = TRUE)

## ASYMMETRIES? ## 
  #QUANTITY-WORD#
      QW<-wilcox.test(Map_HBE$SumTotal_QuantityWord, Map_HBE$SumTotal_WordQuantity, paired = TRUE, exact=FALSE)
      QW
      QW_Zstat<-qnorm(QW$p.value/2)
      QW_Zstat
      abs(QW_Zstat)/sqrt(17) #calculates r value
      
  #QUANTITY-NUMERAL#
      QN<-wilcox.test(Map_HBE$SumTotal_QuantityNumeral, Map_HBE$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE)
      QN
      QN_Zstat<-qnorm(QN$p.value/2)
      QN_Zstat
      abs(QN_Zstat)/sqrt(17)
      
  #WORD-NUMERAL#
      WN<-wilcox.test(Map_HBE$SumTotal_WordNumeral, Map_HBE$SumTotal_NumeralWord, paired = TRUE, exact=FALSE)
      WN
      WN_Zstat<-qnorm(WN$p.value/2)
      WN_Zstat
      abs(WN_Zstat)/sqrt(17)
 
 #Asymmetries x Ages#
QW_56<-wilcox.test(Map56$SumTotal_QuantityWord, Map56$SumTotal_WordQuantity, paired = TRUE, exact=FALSE) #SIG
QW_7up<-wilcox.test(Map7up$SumTotal_QuantityWord, Map7up$SumTotal_WordQuantity, paired = TRUE, exact=FALSE) #SIG
QN_56<-wilcox.test(Map56$SumTotal_QuantityNumeral, Map56$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE) #SIG
QN_7up<-wilcox.test(Map7up$SumTotal_QuantityNumeral, Map7up$SumTotal_NumeralQuantity, paired = TRUE, exact=FALSE) #SIG
WN_56<-wilcox.test(Map56$SumTotal_WordNumeral, Map56$SumTotal_NumeralWord, paired = TRUE, exact=FALSE) #SIG
WN_7up<-wilcox.test(Map7up$SumTotal_WordNumeral, Map7up$SumTotal_NumeralWord, paired = TRUE, exact=FALSE) #SIG
 
 
 ## DEVELOMENTAL TRAJECTORY RESULTS FOR FIGURE##
#5 YO, 1-5 QUANTITIES#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
small$AgeGrp <- ifelse(small$Age_Rounded < 6, "5-year-olds", "6 and up")
sm_Map5 <- subset(small, small$AgeGrp =="5-year-olds")
View(sm_Map5)
#MAPPING PAIR COMPARISONS?#
t.test(sm_Map5$smQN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.37 (QW = QN)
t.test(sm_Map5$smQN, sm_Map5$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.024 (NW > QN)
t.test(sm_Map5$smWN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.019 (NW > QW)
mean(sm_Map5$smQN) #0.82
mean(sm_Map5$smQW) #0.77
mean(sm_Map5$smWN) #0.94
#ASYMMETRIES?# 
#DATA FRAME OF SMALL X MEDIUM FOR EACH MAPPING TYPE#
Small_type <- mutate(Map_HBE, smNQ= AvgCorrect_Sm_QuantityNumeral * AvgCorrect_Med_QuantityNumeral, smQN= AvgCorrect_Sm_NumeralQuantity * AvgCorrect_Med_NumeralQuantity, smNW= AvgCorrect_Sm_WordNumeral * AvgCorrect_Med_WordNumeral, sm_WN= AvgCorrect_Sm_NumeralWord * AvgCorrect_Med_NumeralWord, sm_WQ= AvgCorrect_Sm_QuantityWord, AvgCorrect_Med_QuantityWord, sm_QW= AvgCorrect_Sm_WordQuantity, AvgCorrect_Med_WordQuantity)
View(Small_type)
#5 year olds 1-5#
Small_type$AgeGrp <- ifelse(Small_type$Age_Rounded < 6, "5 year-olds", "6 and up")
sm_Map5_type <- subset(Small_type, Small_type$AgeGrp =="5 year-olds") 
View(sm_Map5_type)
sm_QN_5<-wilcox.test(sm_Map5_type$smQN, sm_Map5_type$smNQ, paired = TRUE, exact=FALSE)
sm_QN_5 #p-value = 0.41 #no asymmetry for QN
sm_NW_5<-wilcox.test(sm_Map5_type$sm_WN, sm_Map5_type$smNW, paired = TRUE, exact=FALSE)
sm_NW_5 #p-value = 0.41 #no asymmetry for NW
sm_QW_5<-wilcox.test(sm_Map5_type$sm_QW, sm_Map5_type$sm_WQ, paired = TRUE, exact=FALSE)
sm_QW_5 #p-value = 0.37 #no asymmetry for QW
#7-9 YO, 1-5 QUANTITIES#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
sm_Map7up <- subset(small, small$AgeGrp =="7 and up")
View(sm_Map7up)
#MAPPING PAIR COMPARISONS?#
t.test(sm_Map7up$smQN,sm_Map7up$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.1613 (QW = QN)
t.test(sm_Map7up$smQN, sm_Map7up$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.409 (NW = QN)
t.test(sm_Map7up$smWN, sm_Map7up$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.4187 (NW = QW)
mean(sm_Map7up$smQN) #0.98
mean(sm_Map7up$smQW) #0.9255952
mean(sm_Map7up$smWN) #0.952381
#7-9 YO, 6-9 QUANTITIES#
#CREATE DF#
large <- mutate(Map_HBE, lrgWN= AvgCorrect_Lrg_WN, lrgQN= AvgCorrect_Lrg_QN, lrgQW= AvgCorrect_Lrg_QW)
View(large)
large$AgeGrp <- ifelse(large$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
large_Map7up <- subset(large, large$AgeGrp =="7 and up")
View(large_Map7up)
#MAPPING PAIR COMPARISONS?#
t.test(large_Map7up$lrgQN,large_Map7up$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.3519 (QW = QN)
t.test(large_Map7up$lrgQN, large_Map7up$lrgWN, paired = TRUE, alternative = "two.sided") #p-value = 0.7252 (NW = QN)
t.test(large_Map7up$lrgWN, large_Map7up$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.2424 (NW = QW)
mean(large_Map7up$lrgWN) #0.9704762
mean(large_Map7up$lrgQW) #0.9357143
mean(large_Map7up$lrgQN) #0.9595238
 #5-6 YO, 1-5 QUANTITIES#
#create quantities 1-5 dataframe, adding new columns#
small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
View(small)
#5 and  6 year olds with quantities 1-5#
small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
sm_Map56 <- subset(small, small$AgeGrp =="5- and 6-year-olds")
View(sm_Map56)
#MAPPING PAIR COMPARISONS#
t.test(sm_Map56$smQN,sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.217 (QW = QN)
t.test(sm_Map56$smQN, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #p-value = 0.007159 (NW > QN)
t.test(sm_Map56$smWN, sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #p-value = 0.004537 (NW > QW)
mean(sm_Map56$smQW) #0.7479762
mean(sm_Map56$smWN) #0.9442857
mean(sm_Map56$smQN) #0.8172619
#5-6 YO, 6-9 QUANTITIES#
#CREATE DF#
large <- mutate(Map_HBE, lrgWN= AvgCorrect_Lrg_WN, lrgQN= AvgCorrect_Lrg_QN, lrgQW= AvgCorrect_Lrg_QW)
View(large)
large$AgeGrp <- ifelse(large$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
large_Map56 <- subset(large, large$AgeGrp =="5- and 6-year-olds")
View(large_Map56)
#MAPPING PAIR COMPARISONS#
t.test(large_Map56$lrgQN,large_Map56$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 0.383 (QW = QN)
t.test(large_Map56$lrgQN,large_Map56$lrgWN, paired = TRUE, alternative = "two.sided")  #p-value = 0.001778 (NW > QN)
t.test(large_Map56$lrgWN,large_Map56$lrgQW, paired = TRUE, alternative = "two.sided") #p-value = 8.263e-05 (NW > QW)
mean(large_Map56$lrgWN) #0.9304762
mean(large_Map56$lrgQN) #0.787619
mean(large_Map56$lrgQW) #0.7580952

 
 
# SCATTER PLOTS OF MAPPING PAIRS # 
#SET JITTER#
jitter <- position_jitter(width=0.7, height=0.04) 
QNy <- Map_HBE$SumTotal_QuantityNumeral
QNx <- Map_HBE$SumTotal_NumeralQuantity
QN <- ggplot(Map_HBE, aes(x=QNx, y=QNy)) + geom_point(position=jitter, size=2)+ labs(x="Quantity-to-Numeral", y="Numeral-to-Quantity")  + ggtitle("Quantity-Numeral") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
QN
# to get r-squared https://rcompanion.org/rcompanion/e_01.html #
QNmodel <- lm(QNy~QNx, data=Map_HBE)
summary(QNmodel) 

QWy <- Map_HBE$SumTotal_QuantityWord
QWx <- Map_HBE$SumTotal_WordQuantity
QW <- ggplot(Map_HBE, aes(x=QWx, y=QWy)) + geom_point(position=jitter, size=2)+ labs(x="Quantity-to-Word", y="Word-to-Quantity") + ggtitle("Quantity-Word") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
QW
QWmodel <- lm(QWy~QWx, data=Map_HBE)
summary(QWmodel) 

NWy <- Map_HBE$SumTotal_NumeralWord
NWx <- Map_HBE$SumTotal_WordNumeral
NW <- ggplot(Map_HBE, aes(x=NWx, y=NWy)) + geom_point(position=jitter, size=2)+ labs(x="Numeral-to-Word", y="Word-to-Numeral") + ggtitle("Numeral-Word") + theme(plot.title = element_text(hjust = 0.5)) + geom_abline() + theme(text = element_text(size=16)) + coord_cartesian(xlim = c(4, 9), ylim= c(4,9))
NW
NWmodel <- lm(NWy~NWx, data=Map_HBE)
summary(NWmodel)
 
 ## HISTOGRAM OF DIFFERENCE SCORES FOR MAPPING PAIRS ##
     #Want columns to start at x axis, but solutions all include changing expand to start at 0. that would remove the blank columns.#
         DH_QW <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Quantity.Word_Word.Quantity)) + geom_bar(aes(y=..count../sum(..count..))) + labs(y="Proportion of Children", x="Quantity-Word > Word-Quantity") + theme(panel.background = element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_QW

         DH_NW <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Numeral.Word_Word.Numeral)) + geom_bar(aes(y=..count../sum(..count..))) + labs(x="Numeral-Word > Word-Numeral") + theme(panel.background = element_blank(), axis.title.y=element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_NW 

         DH_QN <- ggplot(Map_HBE, aes(x=Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity)) + geom_bar(aes(y=..count../sum(..count..))) + labs(x="Quantity-Numeral > Numeral-Quantity")+ theme(panel.background = element_blank(), axis.title.y=element_blank(), text = element_text(size=12, family="Times New Roman")) + expand_limits(x = c(-8,9), y = c(0,1)) + scale_y_continuous(breaks = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)) + scale_x_continuous(breaks=c(-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9)) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.line = element_line(colour = "black")) 
         DH_QN

         grid.arrange(DH_QW, DH_NW, DH_QN, ncol = 3) 
         
     #SKEWNESS, KURTOSIS, MEAN,RANGE,SD,SE#
         install.packages(“psych”)
         library(psych)
         describe(Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity)
         describe(Map_HBE$Difference_Quantity.Word_Word.Quantity)
         describe(Map_HBE$Difference_Numeral.Word_Word.Numeral)
         
         #If skewness is less than -1 or greater than 1, the distribution is highly skewed.
         #If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.
         #If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.
         
     #PERCENT THAT FALLS BETWEEN 1 POINT#
         DiffQW <- Map_HBE$Difference_Quantity.Word_Word.Quantity
         QW_1 <- length(which(DiffQW >= 1 & DiffQW <= 3)) #number of observations that fall 1 point above or below DiffQW median of 2
         N <- nrow(Map_HBE) #gives # of observations
         (QW_1/N)*100

         DiffNW <- Map_HBE$Difference_Numeral.Word_Word.Numeral
         NW_1 <- length(which(DiffNW >= 0 & DiffNW <= 2)) #number of observations that fall 1 point above or below DiffNW median of 1
         N <- nrow(Map_HBE) #gives # of observations
         (NW_1/N)*100

         DiffQN <- Map_HBE$Difference_Quantity.Numeral_Numeral.Quantity
         QN_1 <- length(which(DiffQN >= 0 & DiffQN <= 1)) #number of observations that fall 1 point above or below DiffQN median of 2
         N <- nrow(Map_HBE) #gives # of observations
         (QN_1/N)*100

# TESTING NORMALITY OF DIFFERENCE SCORES # 
shapiro.test(Map_HBE$Difference_Numeral.Word_Word.Numeral)
#ETC FOR TWO OTHER PAIRS


      

    #MAPPING PAIRS COMPARING TO HURST, WITH YOUNGER AGES AND QUANITIES 1-5#
    #create quantities 1-5 dataframe, adding new columns#
    small <- mutate(Map_HBE, smWN= AvgCorrect_Sm_WN * AvgCorrect_Med_WN, smQN= AvgCorrect_Sm_QN * AvgCorrect_Med_QN, smQW= AvgCorrect_Sm_QW * AvgCorrect_Med_QW)
    View(small)
    #5 and  6 year olds with quantities 1-5#
    small$AgeGrp <- ifelse(small$Age_Rounded < 7, "5- and 6-year-olds", "7 and up")
    sm_Map56 <- subset(small, small$AgeGrp =="5- and 6-year-olds")
    View(sm_Map56)
    #5 year olds with quantities 1-5#
    small$AgeGrp <- ifelse(small$Age_Rounded < 6, "5", "6 and up")
    sm_Map5 <- subset(small, small$AgeGrp =="5")
    View(sm_Map5)
    ## QN vs QW ##
    5 and 6 year olds: t.test(sm_Map56$smQN, sm_Map56$smQW, paired = TRUE, alternative = "two.sided") #t = 1.2748, df = 20, p-value = 0.217
    5 year olds: t.test(sm_Map5$smQN, sm_Map5$smQW, paired = TRUE, alternative = "two.sided") #t = 0.92789, df = 11, p-value = 0.3734
    ## QN vs NW ##
    5 and 6 year olds: t.test(sm_Map56$smQN, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #t = -2.9948, df = 20, p-value = 0.007159, NW HIGHER 
    5 year olds: NW HIGHER 
    ## NW vs QW ##
    5 and 6 year olds: t.test(sm_Map56$smQW, sm_Map56$smWN, paired = TRUE, alternative = "two.sided") #t = -3.1961, df = 20, p-value = 0.004537, NW HIGHER
    5 year olds: NW HIGHER


## MAPPING MEANS FOR QUANTITY SIZES ##
      ## QW ##
      mean(Map_HBE$AvgCorrect_Sm_QW)
      mean(Map_HBE$AvgCorrect_Med_QW)
      mean(Map_HBE$AvgCorrect_Lrg_QW)

      ## NW ##
      mean(Map_HBE$AvgCorrect_Sm_WN)
      mean(Map_HBE$AvgCorrect_Med_WN)
      mean(Map_HBE$AvgCorrect_Lrg_WN)

      ## QN ##
      mean(Map_HBE$AvgCorrect_Sm_QN)
      mean(Map_HBE$AvgCorrect_Med_QN)
      mean(Map_HBE$AvgCorrect_Lrg_QN)
       
# SET SIZES T TESTS #
      #GENERAL SMALL VERSUS LARGE DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Lrg_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Lrg_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Lrg_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Lrg_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
      #GENERAL SMALL VERSUS MEDIUM DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Med_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Sm_QW, Map_HBE$AvgCorrect_Med_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Med_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Med_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Sm_WN, Map_HBE$AvgCorrect_Med_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
       #GENERAL MEDIUM VERSUS LARGE DIFFERENCES (NOT AGES)#
      #QW
      t.test(Map_HBE$AvgCorrect_Med_QW, Map_HBE$AvgCorrect_Lrg_QW, paired = FALSE, alternative = "two.sided")
      cohen.d(Map_HBE$AvgCorrect_Med_QW, Map_HBE$AvgCorrect_Lrg_QW,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #QN
      t.test(Map_HBE$AvgCorrect_Med_QN, Map_HBE$AvgCorrect_Lrg_QN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Med_QN, Map_HBE$AvgCorrect_Lrg_QN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE) 
      #NW
      t.test(Map_HBE$AvgCorrect_Med_WN, Map_HBE$AvgCorrect_Lrg_WN, paired = FALSE, alternative = "two.sided") 
      cohen.d(Map_HBE$AvgCorrect_Med_WN, Map_HBE$AvgCorrect_Lrg_WN,pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      
     
      #SMALL VERSUS LARGE#
       #QW
      t.test(Map56$AvgCorrect_Sm_QW, Map56$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_QW, Map56$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_QW, Map7up$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_QW, Map7up$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
     t.test(Map56$AvgCorrect_Sm_QN, Map56$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
     cohen.d(Map56$AvgCorrect_Sm_QN, Map56$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
     t.test(Map7up$AvgCorrect_Sm_QN, Map7up$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
     cohen.d(Map7up$AvgCorrect_Sm_QN, Map7up$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Sm_WN, Map56$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_WN, Map56$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_WN, Map7up$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_WN, Map7up$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       
      #SMALL VERSUS MEDIUM#
       #QW
       t.test(Map56$AvgCorrect_Sm_QW,Map56$AvgCorrect_Med_QW, paired = TRUE, alternative = "two.sided")
       cohen.d(Map56$AvgCorrect_Sm_QW,Map56$AvgCorrect_Med_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
       t.test(Map7up$AvgCorrect_Sm_QW,Map7up$AvgCorrect_Med_QW, paired = TRUE, alternative = "two.sided")
       cohen.d(Map7up$AvgCorrect_Sm_QW,Map7up$AvgCorrect_Med_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
      t.test(Map56$AvgCorrect_Sm_QN,Map56$AvgCorrect_Med_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_QN,Map56$AvgCorrect_Med_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_QN,Map7up$AvgCorrect_Med_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_QN,Map7up$AvgCorrect_Med_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Sm_WN,Map56$AvgCorrect_Med_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Sm_WN,Map56$AvgCorrect_Med_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Sm_WN,Map7up$AvgCorrect_Med_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Sm_WN,Map7up$AvgCorrect_Med_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)

     #MEDIUM VERSUS LARGE#
      #QW
      t.test(Map56$AvgCorrect_Med_QW,Map56$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_QW,Map56$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_QW,Map7up$AvgCorrect_Lrg_QW, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_QW,Map7up$AvgCorrect_Lrg_QW, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #QN
      t.test(Map56$AvgCorrect_Med_QN,Map56$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_QN,Map56$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_QN,Map7up$AvgCorrect_Lrg_QN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_QN,Map7up$AvgCorrect_Lrg_QN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      #NW
      t.test(Map56$AvgCorrect_Med_WN,Map56$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map56$AvgCorrect_Med_WN,Map56$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
      t.test(Map7up$AvgCorrect_Med_WN,Map7up$AvgCorrect_Lrg_WN, paired = TRUE, alternative = "two.sided")
      cohen.d(Map7up$AvgCorrect_Med_WN,Map7up$AvgCorrect_Lrg_WN, pooled=TRUE,paired=FALSE, na.rm=FALSE, hedges.correction=FALSE, conf.level=0.95,noncentral=FALSE)
    
       
    

# T TESTS FOR MAPPING TYPES! #

# 5 and 6 YOs
wilcox.test(Map56$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.54e-05
wilcox.test(Map56$AvgCorrect_Numeral.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.784e-05
wilcox.test(Map56$SumTotal_QuantityNumeral, Map56$SumTotal_NumeralQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity numeral better than numeral quantity? 
#YES, p-value = p-value = 0.001521

wilcox.test(Map56$AvgCorrect_Quantity.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.612e-05
wilcox.test(Map56$AvgCorrect_Word.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 2.956e-05
wilcox.test(Map56$SumTotal_QuantityWord, Map56$SumTotal_WordQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity word better than word quantity?
#YES, p-value = 0.0009956

wilcox.test(Map56$AvgCorrect_Word.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 1.905e-05
wilcox.test(Map56$AvgCorrect_Numeral.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, p-value = 1.396e-05
wilcox.test(Map56$SumTotal_NumeralWord, Map56$SumTotal_WordNumeral, paired=TRUE, exact=FALSE) # ASYMMETRY?  Numeral word better than word numeral?
#YES, p-value = 0.0002018

# 7-9 YOs

wilcox.test(Map7up$AvgCorrect_Quantity.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 7.534e-06
wilcox.test(Map7up$AvgCorrect_Numeral.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 1.204e-05
wilcox.test(Map7up$SumTotal_QuantityNumeral, Map7up$SumTotal_NumeralQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity numeral better than numeral quantity?
#YES, V = 190, p-value = 6.333e-05

wilcox.test(Map7up$AvgCorrect_Quantity.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 7.573e-06
wilcox.test(Map7up$AvgCorrect_Word.Quantity, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 1.925e-05
wilcox.test(Map7up$SumTotal_QuantityWord, Map7up$SumTotal_WordQuantity, paired=TRUE, exact=FALSE) # ASYMMETRY? Quantity word better than word quantity?
#YES, V = 210, p-value = 4.749e-05

wilcox.test(Map7up$AvgCorrect_Word.Numeral, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 210, p-value = 1.255e-05
wilcox.test(Map7up$AvgCorrect_Numeral.Word, mu = .25, alternative = "greater") # is median numeral correct significantly greater than chance (0.25) #YES, V = 231, p-value = 2.525e-06
wilcox.test(Map7up$SumTotal_NumeralWord, Map7up$SumTotal_WordNumeral, paired=TRUE, exact=FALSE) # ASYMMETRY?  Numeral word better than word numeral?
#YES, V = 231, p-value = 1.952e-05
